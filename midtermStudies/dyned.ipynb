{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation Graph and Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expression 5/1\n",
      "[ 1.  2.  3.  4.  5.  6.  7.  8.  6.  8. 10. 12.  2.  3.  4.  5.]\n"
     ]
    }
   ],
   "source": [
    "import dynet as dy\n",
    "\n",
    "dy.renew_cg()\n",
    "\n",
    "v1 = dy.inputVector([1,2,3,4])\n",
    "\n",
    "v2 = dy.inputVector([5,6,7,8])\n",
    "\n",
    "v3 = v1 + v2\n",
    "\n",
    "v4 = v3 * 2\n",
    "\n",
    "v5 = v1 + 1\n",
    "\n",
    "v6 = dy.concatenate([v1,v2,v3,v5])\n",
    "\n",
    "print(v6)\n",
    "\n",
    "print(v6.npvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expression 4/5\n",
      "[-2.05165911 -0.17920166 -0.63342535 -2.28571796  2.33943367  1.44374061\n",
      "  2.40361357  0.15762496  0.33884883 -0.65287077  4.08012819  1.06572986\n",
      " -0.97193158 -1.87402773 -0.03989619  0.8358379  -2.28551912  0.34185609\n",
      " -1.28506875  0.76829171]\n"
     ]
    }
   ],
   "source": [
    "model = dy.Model()\n",
    "\n",
    "pW = model.add_parameters((20,4))\n",
    "pb = model.add_parameters(20)\n",
    "\n",
    "dy.renew_cg()\n",
    "\n",
    "x = dy.inputVector([1,2,3,4])\n",
    "W = dy.parameter(pW)\n",
    "b = dy.parameter(pb)\n",
    "\n",
    "y = W * x + b\n",
    "\n",
    "print(y)\n",
    "\n",
    "print(y.npvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01639034]\n"
     ]
    }
   ],
   "source": [
    "model = dy.Model()\n",
    "\n",
    "trainer = dy.SimpleSGDTrainer(model)\n",
    "\n",
    "p_v = model.add_parameters(10)\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range (EPOCHS):\n",
    "    dy.renew_cg()\n",
    "    \n",
    "    v = dy.parameter(p_v)\n",
    "    v2 = dy.dot_product(v,v)\n",
    "    v2.forward()\n",
    "    \n",
    "    v2.backward()\n",
    "    \n",
    "    trainer.update()\n",
    "    \n",
    "print(v2.npvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. loss = 3.466038\n",
      "Epoch 1. loss = 3.314329\n",
      "Epoch 2. loss = 3.206483\n",
      "Epoch 3. loss = 3.128808\n",
      "Epoch 4. loss = 3.071772\n",
      "Epoch 5. loss = 3.028930\n",
      "Epoch 6. loss = 2.995968\n",
      "Epoch 7. loss = 2.969995\n",
      "Epoch 8. loss = 2.949054\n",
      "Epoch 9. loss = 2.931807\n",
      "Epoch 10. loss = 2.917322\n",
      "Epoch 11. loss = 2.904940\n",
      "Epoch 12. loss = 2.894185\n",
      "Epoch 13. loss = 2.884714\n",
      "Epoch 14. loss = 2.876267\n",
      "Epoch 15. loss = 2.868650\n",
      "Epoch 16. loss = 2.861714\n",
      "Epoch 17. loss = 2.855342\n",
      "Epoch 18. loss = 2.849442\n",
      "Epoch 19. loss = 2.843940\n",
      "Epoch 20. loss = 2.838776\n",
      "Epoch 21. loss = 2.833898\n",
      "Epoch 22. loss = 2.829266\n",
      "Epoch 23. loss = 2.824844\n",
      "Epoch 24. loss = 2.820600\n",
      "Epoch 25. loss = 2.816508\n",
      "Epoch 26. loss = 2.812546\n",
      "Epoch 27. loss = 2.808690\n",
      "Epoch 28. loss = 2.804924\n",
      "Epoch 29. loss = 2.801228\n",
      "Epoch 30. loss = 2.797588\n",
      "Epoch 31. loss = 2.793989\n",
      "Epoch 32. loss = 2.790416\n",
      "Epoch 33. loss = 2.786857\n",
      "Epoch 34. loss = 2.783299\n",
      "Epoch 35. loss = 2.779730\n",
      "Epoch 36. loss = 2.776137\n",
      "Epoch 37. loss = 2.772509\n",
      "Epoch 38. loss = 2.768835\n",
      "Epoch 39. loss = 2.765104\n",
      "Epoch 40. loss = 2.761304\n",
      "Epoch 41. loss = 2.757424\n",
      "Epoch 42. loss = 2.753453\n",
      "Epoch 43. loss = 2.749378\n",
      "Epoch 44. loss = 2.745189\n",
      "Epoch 45. loss = 2.740874\n",
      "Epoch 46. loss = 2.736421\n",
      "Epoch 47. loss = 2.731818\n",
      "Epoch 48. loss = 2.727053\n",
      "Epoch 49. loss = 2.722114\n",
      "Epoch 50. loss = 2.716988\n",
      "Epoch 51. loss = 2.711663\n",
      "Epoch 52. loss = 2.706126\n",
      "Epoch 53. loss = 2.700364\n",
      "Epoch 54. loss = 2.694365\n",
      "Epoch 55. loss = 2.688116\n",
      "Epoch 56. loss = 2.681605\n",
      "Epoch 57. loss = 2.674819\n",
      "Epoch 58. loss = 2.667745\n",
      "Epoch 59. loss = 2.660372\n",
      "Epoch 60. loss = 2.652688\n",
      "Epoch 61. loss = 2.644682\n",
      "Epoch 62. loss = 2.636343\n",
      "Epoch 63. loss = 2.627660\n",
      "Epoch 64. loss = 2.618624\n",
      "Epoch 65. loss = 2.609226\n",
      "Epoch 66. loss = 2.599457\n",
      "Epoch 67. loss = 2.589310\n",
      "Epoch 68. loss = 2.578778\n",
      "Epoch 69. loss = 2.567856\n",
      "Epoch 70. loss = 2.556537\n",
      "Epoch 71. loss = 2.544819\n",
      "Epoch 72. loss = 2.532698\n",
      "Epoch 73. loss = 2.520172\n",
      "Epoch 74. loss = 2.507239\n",
      "Epoch 75. loss = 2.493900\n",
      "Epoch 76. loss = 2.480155\n",
      "Epoch 77. loss = 2.466005\n",
      "Epoch 78. loss = 2.451453\n",
      "Epoch 79. loss = 2.436503\n",
      "Epoch 80. loss = 2.421157\n",
      "Epoch 81. loss = 2.405422\n",
      "Epoch 82. loss = 2.389302\n",
      "Epoch 83. loss = 2.372803\n",
      "Epoch 84. loss = 2.355931\n",
      "Epoch 85. loss = 2.338694\n",
      "Epoch 86. loss = 2.321099\n",
      "Epoch 87. loss = 2.303153\n",
      "Epoch 88. loss = 2.284863\n",
      "Epoch 89. loss = 2.266238\n",
      "Epoch 90. loss = 2.247285\n",
      "Epoch 91. loss = 2.228012\n",
      "Epoch 92. loss = 2.208426\n",
      "Epoch 93. loss = 2.188535\n",
      "Epoch 94. loss = 2.168346\n",
      "Epoch 95. loss = 2.147865\n",
      "Epoch 96. loss = 2.127099\n",
      "Epoch 97. loss = 2.106055\n",
      "Epoch 98. loss = 2.084738\n",
      "Epoch 99. loss = 2.063154\n",
      "Epoch 100. loss = 2.041309\n",
      "Epoch 101. loss = 2.019206\n",
      "Epoch 102. loss = 1.996852\n",
      "Epoch 103. loss = 1.974251\n",
      "Epoch 104. loss = 1.951409\n",
      "Epoch 105. loss = 1.928330\n",
      "Epoch 106. loss = 1.905020\n",
      "Epoch 107. loss = 1.881485\n",
      "Epoch 108. loss = 1.857730\n",
      "Epoch 109. loss = 1.833765\n",
      "Epoch 110. loss = 1.809595\n",
      "Epoch 111. loss = 1.785231\n",
      "Epoch 112. loss = 1.760682\n",
      "Epoch 113. loss = 1.735960\n",
      "Epoch 114. loss = 1.711077\n",
      "Epoch 115. loss = 1.686049\n",
      "Epoch 116. loss = 1.660890\n",
      "Epoch 117. loss = 1.635617\n",
      "Epoch 118. loss = 1.610249\n",
      "Epoch 119. loss = 1.584806\n",
      "Epoch 120. loss = 1.559310\n",
      "Epoch 121. loss = 1.533783\n",
      "Epoch 122. loss = 1.508250\n",
      "Epoch 123. loss = 1.482735\n",
      "Epoch 124. loss = 1.457264\n",
      "Epoch 125. loss = 1.431863\n",
      "Epoch 126. loss = 1.406559\n",
      "Epoch 127. loss = 1.381379\n",
      "Epoch 128. loss = 1.356349\n",
      "Epoch 129. loss = 1.331496\n",
      "Epoch 130. loss = 1.306846\n",
      "Epoch 131. loss = 1.282423\n",
      "Epoch 132. loss = 1.258252\n",
      "Epoch 133. loss = 1.234354\n",
      "Epoch 134. loss = 1.210753\n",
      "Epoch 135. loss = 1.187466\n",
      "Epoch 136. loss = 1.164514\n",
      "Epoch 137. loss = 1.141911\n",
      "Epoch 138. loss = 1.119674\n",
      "Epoch 139. loss = 1.097816\n",
      "Epoch 140. loss = 1.076347\n",
      "Epoch 141. loss = 1.055277\n",
      "Epoch 142. loss = 1.034613\n",
      "Epoch 143. loss = 1.014364\n",
      "Epoch 144. loss = 0.994532\n",
      "Epoch 145. loss = 0.975121\n",
      "Epoch 146. loss = 0.956133\n",
      "Epoch 147. loss = 0.937568\n",
      "Epoch 148. loss = 0.919426\n",
      "Epoch 149. loss = 0.901703\n",
      "Epoch 150. loss = 0.884399\n",
      "Epoch 151. loss = 0.867508\n",
      "Epoch 152. loss = 0.851026\n",
      "Epoch 153. loss = 0.834948\n",
      "Epoch 154. loss = 0.819268\n",
      "Epoch 155. loss = 0.803979\n",
      "Epoch 156. loss = 0.789075\n",
      "Epoch 157. loss = 0.774548\n",
      "Epoch 158. loss = 0.760391\n",
      "Epoch 159. loss = 0.746595\n",
      "Epoch 160. loss = 0.733153\n",
      "Epoch 161. loss = 0.720057\n",
      "Epoch 162. loss = 0.707297\n",
      "Epoch 163. loss = 0.694867\n",
      "Epoch 164. loss = 0.682758\n",
      "Epoch 165. loss = 0.670960\n",
      "Epoch 166. loss = 0.659467\n",
      "Epoch 167. loss = 0.648270\n",
      "Epoch 168. loss = 0.637360\n",
      "Epoch 169. loss = 0.626730\n",
      "Epoch 170. loss = 0.616372\n",
      "Epoch 171. loss = 0.606279\n",
      "Epoch 172. loss = 0.596442\n",
      "Epoch 173. loss = 0.586854\n",
      "Epoch 174. loss = 0.577508\n",
      "Epoch 175. loss = 0.568397\n",
      "Epoch 176. loss = 0.559514\n",
      "Epoch 177. loss = 0.550852\n",
      "Epoch 178. loss = 0.542405\n",
      "Epoch 179. loss = 0.534167\n",
      "Epoch 180. loss = 0.526130\n",
      "Epoch 181. loss = 0.518289\n",
      "Epoch 182. loss = 0.510639\n",
      "Epoch 183. loss = 0.503173\n",
      "Epoch 184. loss = 0.495886\n",
      "Epoch 185. loss = 0.488773\n",
      "Epoch 186. loss = 0.481828\n",
      "Epoch 187. loss = 0.475047\n",
      "Epoch 188. loss = 0.468425\n",
      "Epoch 189. loss = 0.461957\n",
      "Epoch 190. loss = 0.455638\n",
      "Epoch 191. loss = 0.449464\n",
      "Epoch 192. loss = 0.443431\n",
      "Epoch 193. loss = 0.437534\n",
      "Epoch 194. loss = 0.431770\n",
      "Epoch 195. loss = 0.426136\n",
      "Epoch 196. loss = 0.420625\n",
      "Epoch 197. loss = 0.415237\n",
      "Epoch 198. loss = 0.409966\n",
      "Epoch 199. loss = 0.404810\n",
      "Epoch 200. loss = 0.399765\n",
      "Epoch 201. loss = 0.394829\n",
      "Epoch 202. loss = 0.389997\n",
      "Epoch 203. loss = 0.385268\n",
      "Epoch 204. loss = 0.380637\n",
      "Epoch 205. loss = 0.376104\n",
      "Epoch 206. loss = 0.371664\n",
      "Epoch 207. loss = 0.367315\n",
      "Epoch 208. loss = 0.363056\n",
      "Epoch 209. loss = 0.358882\n",
      "Epoch 210. loss = 0.354792\n",
      "Epoch 211. loss = 0.350784\n",
      "Epoch 212. loss = 0.346856\n",
      "Epoch 213. loss = 0.343005\n",
      "Epoch 214. loss = 0.339229\n",
      "Epoch 215. loss = 0.335527\n",
      "Epoch 216. loss = 0.331896\n",
      "Epoch 217. loss = 0.328334\n",
      "Epoch 218. loss = 0.324841\n",
      "Epoch 219. loss = 0.321413\n",
      "Epoch 220. loss = 0.318049\n",
      "Epoch 221. loss = 0.314748\n",
      "Epoch 222. loss = 0.311509\n",
      "Epoch 223. loss = 0.308328\n",
      "Epoch 224. loss = 0.305206\n",
      "Epoch 225. loss = 0.302140\n",
      "Epoch 226. loss = 0.299129\n",
      "Epoch 227. loss = 0.296173\n",
      "Epoch 228. loss = 0.293268\n",
      "Epoch 229. loss = 0.290415\n",
      "Epoch 230. loss = 0.287611\n",
      "Epoch 231. loss = 0.284857\n",
      "Epoch 232. loss = 0.282150\n",
      "Epoch 233. loss = 0.279489\n",
      "Epoch 234. loss = 0.276874\n",
      "Epoch 235. loss = 0.274303\n",
      "Epoch 236. loss = 0.271775\n",
      "Epoch 237. loss = 0.269289\n",
      "Epoch 238. loss = 0.266845\n",
      "Epoch 239. loss = 0.264441\n",
      "Epoch 240. loss = 0.262076\n",
      "Epoch 241. loss = 0.259750\n",
      "Epoch 242. loss = 0.257461\n",
      "Epoch 243. loss = 0.255209\n",
      "Epoch 244. loss = 0.252993\n",
      "Epoch 245. loss = 0.250812\n",
      "Epoch 246. loss = 0.248666\n",
      "Epoch 247. loss = 0.246553\n",
      "Epoch 248. loss = 0.244473\n",
      "Epoch 249. loss = 0.242425\n",
      "Epoch 250. loss = 0.240408\n",
      "Epoch 251. loss = 0.238422\n",
      "Epoch 252. loss = 0.236466\n",
      "Epoch 253. loss = 0.234540\n",
      "Epoch 254. loss = 0.232643\n",
      "Epoch 255. loss = 0.230773\n",
      "Epoch 256. loss = 0.228932\n",
      "Epoch 257. loss = 0.227117\n",
      "Epoch 258. loss = 0.225330\n",
      "Epoch 259. loss = 0.223567\n",
      "Epoch 260. loss = 0.221831\n",
      "Epoch 261. loss = 0.220119\n",
      "Epoch 262. loss = 0.218432\n",
      "Epoch 263. loss = 0.216768\n",
      "Epoch 264. loss = 0.215129\n",
      "Epoch 265. loss = 0.213512\n",
      "Epoch 266. loss = 0.211917\n",
      "Epoch 267. loss = 0.210345\n",
      "Epoch 268. loss = 0.208794\n",
      "Epoch 269. loss = 0.207264\n",
      "Epoch 270. loss = 0.205756\n",
      "Epoch 271. loss = 0.204267\n",
      "Epoch 272. loss = 0.202799\n",
      "Epoch 273. loss = 0.201350\n",
      "Epoch 274. loss = 0.199921\n",
      "Epoch 275. loss = 0.198510\n",
      "Epoch 276. loss = 0.197118\n",
      "Epoch 277. loss = 0.195744\n",
      "Epoch 278. loss = 0.194388\n",
      "Epoch 279. loss = 0.193050\n",
      "Epoch 280. loss = 0.191729\n",
      "Epoch 281. loss = 0.190424\n",
      "Epoch 282. loss = 0.189136\n",
      "Epoch 283. loss = 0.187865\n",
      "Epoch 284. loss = 0.186609\n",
      "Epoch 285. loss = 0.185369\n",
      "Epoch 286. loss = 0.184145\n",
      "Epoch 287. loss = 0.182936\n",
      "Epoch 288. loss = 0.181741\n",
      "Epoch 289. loss = 0.180561\n",
      "Epoch 290. loss = 0.179396\n",
      "Epoch 291. loss = 0.178245\n",
      "Epoch 292. loss = 0.177107\n",
      "Epoch 293. loss = 0.175983\n",
      "Epoch 294. loss = 0.174873\n",
      "Epoch 295. loss = 0.173776\n",
      "Epoch 296. loss = 0.172692\n",
      "Epoch 297. loss = 0.171620\n",
      "Epoch 298. loss = 0.170561\n",
      "Epoch 299. loss = 0.169514\n",
      "Epoch 300. loss = 0.168479\n",
      "Epoch 301. loss = 0.167456\n",
      "Epoch 302. loss = 0.166445\n",
      "Epoch 303. loss = 0.165446\n",
      "Epoch 304. loss = 0.164457\n",
      "Epoch 305. loss = 0.163480\n",
      "Epoch 306. loss = 0.162514\n",
      "Epoch 307. loss = 0.161558\n",
      "Epoch 308. loss = 0.160614\n",
      "Epoch 309. loss = 0.159679\n",
      "Epoch 310. loss = 0.158755\n",
      "Epoch 311. loss = 0.157841\n",
      "Epoch 312. loss = 0.156937\n",
      "Epoch 313. loss = 0.156043\n",
      "Epoch 314. loss = 0.155158\n",
      "Epoch 315. loss = 0.154283\n",
      "Epoch 316. loss = 0.153417\n",
      "Epoch 317. loss = 0.152561\n",
      "Epoch 318. loss = 0.151713\n",
      "Epoch 319. loss = 0.150874\n",
      "Epoch 320. loss = 0.150045\n",
      "Epoch 321. loss = 0.149223\n",
      "Epoch 322. loss = 0.148411\n",
      "Epoch 323. loss = 0.147606\n",
      "Epoch 324. loss = 0.146810\n",
      "Epoch 325. loss = 0.146023\n",
      "Epoch 326. loss = 0.145243\n",
      "Epoch 327. loss = 0.144471\n",
      "Epoch 328. loss = 0.143707\n",
      "Epoch 329. loss = 0.142950\n",
      "Epoch 330. loss = 0.142201\n",
      "Epoch 331. loss = 0.141460\n",
      "Epoch 332. loss = 0.140726\n",
      "Epoch 333. loss = 0.139999\n",
      "Epoch 334. loss = 0.139280\n",
      "Epoch 335. loss = 0.138567\n",
      "Epoch 336. loss = 0.137861\n",
      "Epoch 337. loss = 0.137162\n",
      "Epoch 338. loss = 0.136470\n",
      "Epoch 339. loss = 0.135785\n",
      "Epoch 340. loss = 0.135106\n",
      "Epoch 341. loss = 0.134433\n",
      "Epoch 342. loss = 0.133767\n",
      "Epoch 343. loss = 0.133107\n",
      "Epoch 344. loss = 0.132454\n",
      "Epoch 345. loss = 0.131807\n",
      "Epoch 346. loss = 0.131165\n",
      "Epoch 347. loss = 0.130530\n",
      "Epoch 348. loss = 0.129900\n",
      "Epoch 349. loss = 0.129276\n",
      "Epoch 350. loss = 0.128658\n",
      "Epoch 351. loss = 0.128046\n",
      "Epoch 352. loss = 0.127439\n",
      "Epoch 353. loss = 0.126838\n",
      "Epoch 354. loss = 0.126242\n",
      "Epoch 355. loss = 0.125651\n",
      "Epoch 356. loss = 0.125066\n",
      "Epoch 357. loss = 0.124486\n",
      "Epoch 358. loss = 0.123911\n",
      "Epoch 359. loss = 0.123341\n",
      "Epoch 360. loss = 0.122777\n",
      "Epoch 361. loss = 0.122217\n",
      "Epoch 362. loss = 0.121662\n",
      "Epoch 363. loss = 0.121112\n",
      "Epoch 364. loss = 0.120566\n",
      "Epoch 365. loss = 0.120026\n",
      "Epoch 366. loss = 0.119490\n",
      "Epoch 367. loss = 0.118958\n",
      "Epoch 368. loss = 0.118431\n",
      "Epoch 369. loss = 0.117909\n",
      "Epoch 370. loss = 0.117391\n",
      "Epoch 371. loss = 0.116877\n",
      "Epoch 372. loss = 0.116368\n",
      "Epoch 373. loss = 0.115863\n",
      "Epoch 374. loss = 0.115362\n",
      "Epoch 375. loss = 0.114865\n",
      "Epoch 376. loss = 0.114373\n",
      "Epoch 377. loss = 0.113884\n",
      "Epoch 378. loss = 0.113399\n",
      "Epoch 379. loss = 0.112919\n",
      "Epoch 380. loss = 0.112442\n",
      "Epoch 381. loss = 0.111969\n",
      "Epoch 382. loss = 0.111501\n",
      "Epoch 383. loss = 0.111035\n",
      "Epoch 384. loss = 0.110574\n",
      "Epoch 385. loss = 0.110116\n",
      "Epoch 386. loss = 0.109662\n",
      "Epoch 387. loss = 0.109211\n",
      "Epoch 388. loss = 0.108764\n",
      "Epoch 389. loss = 0.108321\n",
      "Epoch 390. loss = 0.107881\n",
      "Epoch 391. loss = 0.107444\n",
      "Epoch 392. loss = 0.107011\n",
      "Epoch 393. loss = 0.106581\n",
      "Epoch 394. loss = 0.106155\n",
      "Epoch 395. loss = 0.105732\n",
      "Epoch 396. loss = 0.105312\n",
      "Epoch 397. loss = 0.104895\n",
      "Epoch 398. loss = 0.104482\n",
      "Epoch 399. loss = 0.104071\n",
      "Epoch 400. loss = 0.103664\n",
      "Epoch 401. loss = 0.103260\n",
      "Epoch 402. loss = 0.102858\n",
      "Epoch 403. loss = 0.102460\n",
      "Epoch 404. loss = 0.102065\n",
      "Epoch 405. loss = 0.101673\n",
      "Epoch 406. loss = 0.101284\n",
      "Epoch 407. loss = 0.100897\n",
      "Epoch 408. loss = 0.100514\n",
      "Epoch 409. loss = 0.100133\n",
      "Epoch 410. loss = 0.099755\n",
      "Epoch 411. loss = 0.099379\n",
      "Epoch 412. loss = 0.099007\n",
      "Epoch 413. loss = 0.098637\n",
      "Epoch 414. loss = 0.098270\n",
      "Epoch 415. loss = 0.097905\n",
      "Epoch 416. loss = 0.097543\n",
      "Epoch 417. loss = 0.097184\n",
      "Epoch 418. loss = 0.096827\n",
      "Epoch 419. loss = 0.096473\n",
      "Epoch 420. loss = 0.096121\n",
      "Epoch 421. loss = 0.095772\n",
      "Epoch 422. loss = 0.095425\n",
      "Epoch 423. loss = 0.095081\n",
      "Epoch 424. loss = 0.094739\n",
      "Epoch 425. loss = 0.094399\n",
      "Epoch 426. loss = 0.094062\n",
      "Epoch 427. loss = 0.093727\n",
      "Epoch 428. loss = 0.093394\n",
      "Epoch 429. loss = 0.093064\n",
      "Epoch 430. loss = 0.092736\n",
      "Epoch 431. loss = 0.092410\n",
      "Epoch 432. loss = 0.092087\n",
      "Epoch 433. loss = 0.091765\n",
      "Epoch 434. loss = 0.091446\n",
      "Epoch 435. loss = 0.091129\n",
      "Epoch 436. loss = 0.090814\n",
      "Epoch 437. loss = 0.090501\n",
      "Epoch 438. loss = 0.090190\n",
      "Epoch 439. loss = 0.089881\n",
      "Epoch 440. loss = 0.089575\n",
      "Epoch 441. loss = 0.089270\n",
      "Epoch 442. loss = 0.088967\n",
      "Epoch 443. loss = 0.088667\n",
      "Epoch 444. loss = 0.088368\n",
      "Epoch 445. loss = 0.088071\n",
      "Epoch 446. loss = 0.087777\n",
      "Epoch 447. loss = 0.087484\n",
      "Epoch 448. loss = 0.087193\n",
      "Epoch 449. loss = 0.086904\n",
      "Epoch 450. loss = 0.086616\n",
      "Epoch 451. loss = 0.086331\n",
      "Epoch 452. loss = 0.086047\n",
      "Epoch 453. loss = 0.085766\n",
      "Epoch 454. loss = 0.085486\n",
      "Epoch 455. loss = 0.085207\n",
      "Epoch 456. loss = 0.084931\n",
      "Epoch 457. loss = 0.084656\n",
      "Epoch 458. loss = 0.084383\n",
      "Epoch 459. loss = 0.084112\n",
      "Epoch 460. loss = 0.083842\n",
      "Epoch 461. loss = 0.083574\n",
      "Epoch 462. loss = 0.083308\n",
      "Epoch 463. loss = 0.083044\n",
      "Epoch 464. loss = 0.082781\n",
      "Epoch 465. loss = 0.082519\n",
      "Epoch 466. loss = 0.082259\n",
      "Epoch 467. loss = 0.082001\n",
      "Epoch 468. loss = 0.081745\n",
      "Epoch 469. loss = 0.081490\n",
      "Epoch 470. loss = 0.081236\n",
      "Epoch 471. loss = 0.080984\n",
      "Epoch 472. loss = 0.080734\n",
      "Epoch 473. loss = 0.080485\n",
      "Epoch 474. loss = 0.080237\n",
      "Epoch 475. loss = 0.079991\n",
      "Epoch 476. loss = 0.079747\n",
      "Epoch 477. loss = 0.079504\n",
      "Epoch 478. loss = 0.079262\n",
      "Epoch 479. loss = 0.079022\n",
      "Epoch 480. loss = 0.078783\n",
      "Epoch 481. loss = 0.078546\n",
      "Epoch 482. loss = 0.078310\n",
      "Epoch 483. loss = 0.078076\n",
      "Epoch 484. loss = 0.077842\n",
      "Epoch 485. loss = 0.077611\n",
      "Epoch 486. loss = 0.077380\n",
      "Epoch 487. loss = 0.077151\n",
      "Epoch 488. loss = 0.076923\n",
      "Epoch 489. loss = 0.076696\n",
      "Epoch 490. loss = 0.076471\n",
      "Epoch 491. loss = 0.076247\n",
      "Epoch 492. loss = 0.076024\n",
      "Epoch 493. loss = 0.075803\n",
      "Epoch 494. loss = 0.075583\n",
      "Epoch 495. loss = 0.075364\n",
      "Epoch 496. loss = 0.075146\n",
      "Epoch 497. loss = 0.074930\n",
      "Epoch 498. loss = 0.074714\n",
      "Epoch 499. loss = 0.074500\n",
      "Epoch 500. loss = 0.074288\n",
      "Epoch 501. loss = 0.074076\n",
      "Epoch 502. loss = 0.073865\n",
      "Epoch 503. loss = 0.073656\n",
      "Epoch 504. loss = 0.073448\n",
      "Epoch 505. loss = 0.073241\n",
      "Epoch 506. loss = 0.073035\n",
      "Epoch 507. loss = 0.072830\n",
      "Epoch 508. loss = 0.072627\n",
      "Epoch 509. loss = 0.072424\n",
      "Epoch 510. loss = 0.072223\n",
      "Epoch 511. loss = 0.072022\n",
      "Epoch 512. loss = 0.071823\n",
      "Epoch 513. loss = 0.071625\n",
      "Epoch 514. loss = 0.071428\n",
      "Epoch 515. loss = 0.071232\n",
      "Epoch 516. loss = 0.071037\n",
      "Epoch 517. loss = 0.070843\n",
      "Epoch 518. loss = 0.070650\n",
      "Epoch 519. loss = 0.070458\n",
      "Epoch 520. loss = 0.070267\n",
      "Epoch 521. loss = 0.070077\n",
      "Epoch 522. loss = 0.069888\n",
      "Epoch 523. loss = 0.069700\n",
      "Epoch 524. loss = 0.069513\n",
      "Epoch 525. loss = 0.069327\n",
      "Epoch 526. loss = 0.069142\n",
      "Epoch 527. loss = 0.068958\n",
      "Epoch 528. loss = 0.068775\n",
      "Epoch 529. loss = 0.068593\n",
      "Epoch 530. loss = 0.068412\n",
      "Epoch 531. loss = 0.068232\n",
      "Epoch 532. loss = 0.068053\n",
      "Epoch 533. loss = 0.067874\n",
      "Epoch 534. loss = 0.067697\n",
      "Epoch 535. loss = 0.067520\n",
      "Epoch 536. loss = 0.067345\n",
      "Epoch 537. loss = 0.067170\n",
      "Epoch 538. loss = 0.066996\n",
      "Epoch 539. loss = 0.066823\n",
      "Epoch 540. loss = 0.066651\n",
      "Epoch 541. loss = 0.066480\n",
      "Epoch 542. loss = 0.066309\n",
      "Epoch 543. loss = 0.066139\n",
      "Epoch 544. loss = 0.065971\n",
      "Epoch 545. loss = 0.065803\n",
      "Epoch 546. loss = 0.065636\n",
      "Epoch 547. loss = 0.065470\n",
      "Epoch 548. loss = 0.065304\n",
      "Epoch 549. loss = 0.065140\n",
      "Epoch 550. loss = 0.064976\n",
      "Epoch 551. loss = 0.064813\n",
      "Epoch 552. loss = 0.064651\n",
      "Epoch 553. loss = 0.064490\n",
      "Epoch 554. loss = 0.064329\n",
      "Epoch 555. loss = 0.064169\n",
      "Epoch 556. loss = 0.064010\n",
      "Epoch 557. loss = 0.063852\n",
      "Epoch 558. loss = 0.063695\n",
      "Epoch 559. loss = 0.063538\n",
      "Epoch 560. loss = 0.063382\n",
      "Epoch 561. loss = 0.063227\n",
      "Epoch 562. loss = 0.063072\n",
      "Epoch 563. loss = 0.062919\n",
      "Epoch 564. loss = 0.062766\n",
      "Epoch 565. loss = 0.062613\n",
      "Epoch 566. loss = 0.062462\n",
      "Epoch 567. loss = 0.062311\n",
      "Epoch 568. loss = 0.062161\n",
      "Epoch 569. loss = 0.062012\n",
      "Epoch 570. loss = 0.061863\n",
      "Epoch 571. loss = 0.061715\n",
      "Epoch 572. loss = 0.061567\n",
      "Epoch 573. loss = 0.061421\n",
      "Epoch 574. loss = 0.061275\n",
      "Epoch 575. loss = 0.061130\n",
      "Epoch 576. loss = 0.060985\n",
      "Epoch 577. loss = 0.060841\n",
      "Epoch 578. loss = 0.060698\n",
      "Epoch 579. loss = 0.060555\n",
      "Epoch 580. loss = 0.060413\n",
      "Epoch 581. loss = 0.060272\n",
      "Epoch 582. loss = 0.060132\n",
      "Epoch 583. loss = 0.059991\n",
      "Epoch 584. loss = 0.059852\n",
      "Epoch 585. loss = 0.059713\n",
      "Epoch 586. loss = 0.059575\n",
      "Epoch 587. loss = 0.059438\n",
      "Epoch 588. loss = 0.059301\n",
      "Epoch 589. loss = 0.059165\n",
      "Epoch 590. loss = 0.059029\n",
      "Epoch 591. loss = 0.058894\n",
      "Epoch 592. loss = 0.058760\n",
      "Epoch 593. loss = 0.058626\n",
      "Epoch 594. loss = 0.058493\n",
      "Epoch 595. loss = 0.058360\n",
      "Epoch 596. loss = 0.058228\n",
      "Epoch 597. loss = 0.058096\n",
      "Epoch 598. loss = 0.057966\n",
      "Epoch 599. loss = 0.057835\n",
      "Epoch 600. loss = 0.057706\n",
      "Epoch 601. loss = 0.057577\n",
      "Epoch 602. loss = 0.057448\n",
      "Epoch 603. loss = 0.057320\n",
      "Epoch 604. loss = 0.057193\n",
      "Epoch 605. loss = 0.057066\n",
      "Epoch 606. loss = 0.056939\n",
      "Epoch 607. loss = 0.056813\n",
      "Epoch 608. loss = 0.056688\n",
      "Epoch 609. loss = 0.056564\n",
      "Epoch 610. loss = 0.056439\n",
      "Epoch 611. loss = 0.056316\n",
      "Epoch 612. loss = 0.056193\n",
      "Epoch 613. loss = 0.056070\n",
      "Epoch 614. loss = 0.055948\n",
      "Epoch 615. loss = 0.055826\n",
      "Epoch 616. loss = 0.055705\n",
      "Epoch 617. loss = 0.055585\n",
      "Epoch 618. loss = 0.055465\n",
      "Epoch 619. loss = 0.055345\n",
      "Epoch 620. loss = 0.055226\n",
      "Epoch 621. loss = 0.055108\n",
      "Epoch 622. loss = 0.054990\n",
      "Epoch 623. loss = 0.054872\n",
      "Epoch 624. loss = 0.054755\n",
      "Epoch 625. loss = 0.054639\n",
      "Epoch 626. loss = 0.054523\n",
      "Epoch 627. loss = 0.054407\n",
      "Epoch 628. loss = 0.054292\n",
      "Epoch 629. loss = 0.054178\n",
      "Epoch 630. loss = 0.054064\n",
      "Epoch 631. loss = 0.053950\n",
      "Epoch 632. loss = 0.053837\n",
      "Epoch 633. loss = 0.053724\n",
      "Epoch 634. loss = 0.053612\n",
      "Epoch 635. loss = 0.053500\n",
      "Epoch 636. loss = 0.053389\n",
      "Epoch 637. loss = 0.053278\n",
      "Epoch 638. loss = 0.053168\n",
      "Epoch 639. loss = 0.053058\n",
      "Epoch 640. loss = 0.052948\n",
      "Epoch 641. loss = 0.052839\n",
      "Epoch 642. loss = 0.052730\n",
      "Epoch 643. loss = 0.052622\n",
      "Epoch 644. loss = 0.052514\n",
      "Epoch 645. loss = 0.052407\n",
      "Epoch 646. loss = 0.052300\n",
      "Epoch 647. loss = 0.052194\n",
      "Epoch 648. loss = 0.052088\n",
      "Epoch 649. loss = 0.051982\n",
      "Epoch 650. loss = 0.051877\n",
      "Epoch 651. loss = 0.051772\n",
      "Epoch 652. loss = 0.051668\n",
      "Epoch 653. loss = 0.051564\n",
      "Epoch 654. loss = 0.051460\n",
      "Epoch 655. loss = 0.051357\n",
      "Epoch 656. loss = 0.051254\n",
      "Epoch 657. loss = 0.051152\n",
      "Epoch 658. loss = 0.051050\n",
      "Epoch 659. loss = 0.050949\n",
      "Epoch 660. loss = 0.050848\n",
      "Epoch 661. loss = 0.050747\n",
      "Epoch 662. loss = 0.050647\n",
      "Epoch 663. loss = 0.050547\n",
      "Epoch 664. loss = 0.050447\n",
      "Epoch 665. loss = 0.050348\n",
      "Epoch 666. loss = 0.050249\n",
      "Epoch 667. loss = 0.050151\n",
      "Epoch 668. loss = 0.050053\n",
      "Epoch 669. loss = 0.049955\n",
      "Epoch 670. loss = 0.049858\n",
      "Epoch 671. loss = 0.049761\n",
      "Epoch 672. loss = 0.049664\n",
      "Epoch 673. loss = 0.049568\n",
      "Epoch 674. loss = 0.049472\n",
      "Epoch 675. loss = 0.049377\n",
      "Epoch 676. loss = 0.049282\n",
      "Epoch 677. loss = 0.049187\n",
      "Epoch 678. loss = 0.049093\n",
      "Epoch 679. loss = 0.048999\n",
      "Epoch 680. loss = 0.048905\n",
      "Epoch 681. loss = 0.048812\n",
      "Epoch 682. loss = 0.048719\n",
      "Epoch 683. loss = 0.048626\n",
      "Epoch 684. loss = 0.048534\n",
      "Epoch 685. loss = 0.048442\n",
      "Epoch 686. loss = 0.048351\n",
      "Epoch 687. loss = 0.048259\n",
      "Epoch 688. loss = 0.048168\n",
      "Epoch 689. loss = 0.048078\n",
      "Epoch 690. loss = 0.047988\n",
      "Epoch 691. loss = 0.047898\n",
      "Epoch 692. loss = 0.047808\n",
      "Epoch 693. loss = 0.047719\n",
      "Epoch 694. loss = 0.047630\n",
      "Epoch 695. loss = 0.047541\n",
      "Epoch 696. loss = 0.047453\n",
      "Epoch 697. loss = 0.047365\n",
      "Epoch 698. loss = 0.047278\n",
      "Epoch 699. loss = 0.047190\n",
      "Epoch 700. loss = 0.047103\n",
      "Epoch 701. loss = 0.047017\n",
      "Epoch 702. loss = 0.046930\n",
      "Epoch 703. loss = 0.046844\n",
      "Epoch 704. loss = 0.046759\n",
      "Epoch 705. loss = 0.046673\n",
      "Epoch 706. loss = 0.046588\n",
      "Epoch 707. loss = 0.046503\n",
      "Epoch 708. loss = 0.046419\n",
      "Epoch 709. loss = 0.046335\n",
      "Epoch 710. loss = 0.046251\n",
      "Epoch 711. loss = 0.046167\n",
      "Epoch 712. loss = 0.046084\n",
      "Epoch 713. loss = 0.046001\n",
      "Epoch 714. loss = 0.045918\n",
      "Epoch 715. loss = 0.045836\n",
      "Epoch 716. loss = 0.045754\n",
      "Epoch 717. loss = 0.045672\n",
      "Epoch 718. loss = 0.045590\n",
      "Epoch 719. loss = 0.045509\n",
      "Epoch 720. loss = 0.045428\n",
      "Epoch 721. loss = 0.045347\n",
      "Epoch 722. loss = 0.045267\n",
      "Epoch 723. loss = 0.045187\n",
      "Epoch 724. loss = 0.045107\n",
      "Epoch 725. loss = 0.045027\n",
      "Epoch 726. loss = 0.044948\n",
      "Epoch 727. loss = 0.044869\n",
      "Epoch 728. loss = 0.044790\n",
      "Epoch 729. loss = 0.044712\n",
      "Epoch 730. loss = 0.044634\n",
      "Epoch 731. loss = 0.044556\n",
      "Epoch 732. loss = 0.044478\n",
      "Epoch 733. loss = 0.044401\n",
      "Epoch 734. loss = 0.044324\n",
      "Epoch 735. loss = 0.044247\n",
      "Epoch 736. loss = 0.044170\n",
      "Epoch 737. loss = 0.044094\n",
      "Epoch 738. loss = 0.044018\n",
      "Epoch 739. loss = 0.043942\n",
      "Epoch 740. loss = 0.043866\n",
      "Epoch 741. loss = 0.043791\n",
      "Epoch 742. loss = 0.043716\n",
      "Epoch 743. loss = 0.043641\n",
      "Epoch 744. loss = 0.043566\n",
      "Epoch 745. loss = 0.043492\n",
      "Epoch 746. loss = 0.043418\n",
      "Epoch 747. loss = 0.043344\n",
      "Epoch 748. loss = 0.043271\n",
      "Epoch 749. loss = 0.043197\n",
      "Epoch 750. loss = 0.043124\n",
      "Epoch 751. loss = 0.043052\n",
      "Epoch 752. loss = 0.042979\n",
      "Epoch 753. loss = 0.042907\n",
      "Epoch 754. loss = 0.042835\n",
      "Epoch 755. loss = 0.042763\n",
      "Epoch 756. loss = 0.042691\n",
      "Epoch 757. loss = 0.042620\n",
      "Epoch 758. loss = 0.042548\n",
      "Epoch 759. loss = 0.042478\n",
      "Epoch 760. loss = 0.042407\n",
      "Epoch 761. loss = 0.042336\n",
      "Epoch 762. loss = 0.042266\n",
      "Epoch 763. loss = 0.042196\n",
      "Epoch 764. loss = 0.042126\n",
      "Epoch 765. loss = 0.042057\n",
      "Epoch 766. loss = 0.041988\n",
      "Epoch 767. loss = 0.041918\n",
      "Epoch 768. loss = 0.041850\n",
      "Epoch 769. loss = 0.041781\n",
      "Epoch 770. loss = 0.041713\n",
      "Epoch 771. loss = 0.041644\n",
      "Epoch 772. loss = 0.041576\n",
      "Epoch 773. loss = 0.041509\n",
      "Epoch 774. loss = 0.041441\n",
      "Epoch 775. loss = 0.041374\n",
      "Epoch 776. loss = 0.041307\n",
      "Epoch 777. loss = 0.041240\n",
      "Epoch 778. loss = 0.041173\n",
      "Epoch 779. loss = 0.041107\n",
      "Epoch 780. loss = 0.041040\n",
      "Epoch 781. loss = 0.040974\n",
      "Epoch 782. loss = 0.040909\n",
      "Epoch 783. loss = 0.040843\n",
      "Epoch 784. loss = 0.040777\n",
      "Epoch 785. loss = 0.040712\n",
      "Epoch 786. loss = 0.040647\n",
      "Epoch 787. loss = 0.040582\n",
      "Epoch 788. loss = 0.040518\n",
      "Epoch 789. loss = 0.040453\n",
      "Epoch 790. loss = 0.040389\n",
      "Epoch 791. loss = 0.040325\n",
      "Epoch 792. loss = 0.040261\n",
      "Epoch 793. loss = 0.040198\n",
      "Epoch 794. loss = 0.040134\n",
      "Epoch 795. loss = 0.040071\n",
      "Epoch 796. loss = 0.040008\n",
      "Epoch 797. loss = 0.039945\n",
      "Epoch 798. loss = 0.039883\n",
      "Epoch 799. loss = 0.039820\n",
      "Epoch 800. loss = 0.039758\n",
      "Epoch 801. loss = 0.039696\n",
      "Epoch 802. loss = 0.039634\n",
      "Epoch 803. loss = 0.039572\n",
      "Epoch 804. loss = 0.039511\n",
      "Epoch 805. loss = 0.039450\n",
      "Epoch 806. loss = 0.039389\n",
      "Epoch 807. loss = 0.039328\n",
      "Epoch 808. loss = 0.039267\n",
      "Epoch 809. loss = 0.039207\n",
      "Epoch 810. loss = 0.039146\n",
      "Epoch 811. loss = 0.039086\n",
      "Epoch 812. loss = 0.039026\n",
      "Epoch 813. loss = 0.038966\n",
      "Epoch 814. loss = 0.038906\n",
      "Epoch 815. loss = 0.038847\n",
      "Epoch 816. loss = 0.038788\n",
      "Epoch 817. loss = 0.038729\n",
      "Epoch 818. loss = 0.038670\n",
      "Epoch 819. loss = 0.038611\n",
      "Epoch 820. loss = 0.038552\n",
      "Epoch 821. loss = 0.038494\n",
      "Epoch 822. loss = 0.038436\n",
      "Epoch 823. loss = 0.038378\n",
      "Epoch 824. loss = 0.038320\n",
      "Epoch 825. loss = 0.038262\n",
      "Epoch 826. loss = 0.038205\n",
      "Epoch 827. loss = 0.038148\n",
      "Epoch 828. loss = 0.038090\n",
      "Epoch 829. loss = 0.038033\n",
      "Epoch 830. loss = 0.037976\n",
      "Epoch 831. loss = 0.037920\n",
      "Epoch 832. loss = 0.037863\n",
      "Epoch 833. loss = 0.037807\n",
      "Epoch 834. loss = 0.037751\n",
      "Epoch 835. loss = 0.037695\n",
      "Epoch 836. loss = 0.037639\n",
      "Epoch 837. loss = 0.037583\n",
      "Epoch 838. loss = 0.037528\n",
      "Epoch 839. loss = 0.037472\n",
      "Epoch 840. loss = 0.037417\n",
      "Epoch 841. loss = 0.037362\n",
      "Epoch 842. loss = 0.037307\n",
      "Epoch 843. loss = 0.037253\n",
      "Epoch 844. loss = 0.037198\n",
      "Epoch 845. loss = 0.037144\n",
      "Epoch 846. loss = 0.037089\n",
      "Epoch 847. loss = 0.037035\n",
      "Epoch 848. loss = 0.036981\n",
      "Epoch 849. loss = 0.036928\n",
      "Epoch 850. loss = 0.036874\n",
      "Epoch 851. loss = 0.036820\n",
      "Epoch 852. loss = 0.036767\n",
      "Epoch 853. loss = 0.036714\n",
      "Epoch 854. loss = 0.036661\n",
      "Epoch 855. loss = 0.036608\n",
      "Epoch 856. loss = 0.036555\n",
      "Epoch 857. loss = 0.036503\n",
      "Epoch 858. loss = 0.036450\n",
      "Epoch 859. loss = 0.036398\n",
      "Epoch 860. loss = 0.036346\n",
      "Epoch 861. loss = 0.036294\n",
      "Epoch 862. loss = 0.036242\n",
      "Epoch 863. loss = 0.036191\n",
      "Epoch 864. loss = 0.036139\n",
      "Epoch 865. loss = 0.036088\n",
      "Epoch 866. loss = 0.036036\n",
      "Epoch 867. loss = 0.035985\n",
      "Epoch 868. loss = 0.035934\n",
      "Epoch 869. loss = 0.035884\n",
      "Epoch 870. loss = 0.035833\n",
      "Epoch 871. loss = 0.035783\n",
      "Epoch 872. loss = 0.035732\n",
      "Epoch 873. loss = 0.035682\n",
      "Epoch 874. loss = 0.035632\n",
      "Epoch 875. loss = 0.035582\n",
      "Epoch 876. loss = 0.035532\n",
      "Epoch 877. loss = 0.035482\n",
      "Epoch 878. loss = 0.035433\n",
      "Epoch 879. loss = 0.035383\n",
      "Epoch 880. loss = 0.035334\n",
      "Epoch 881. loss = 0.035285\n",
      "Epoch 882. loss = 0.035236\n",
      "Epoch 883. loss = 0.035187\n",
      "Epoch 884. loss = 0.035138\n",
      "Epoch 885. loss = 0.035090\n",
      "Epoch 886. loss = 0.035041\n",
      "Epoch 887. loss = 0.034993\n",
      "Epoch 888. loss = 0.034945\n",
      "Epoch 889. loss = 0.034897\n",
      "Epoch 890. loss = 0.034849\n",
      "Epoch 891. loss = 0.034801\n",
      "Epoch 892. loss = 0.034753\n",
      "Epoch 893. loss = 0.034706\n",
      "Epoch 894. loss = 0.034658\n",
      "Epoch 895. loss = 0.034611\n",
      "Epoch 896. loss = 0.034564\n",
      "Epoch 897. loss = 0.034516\n",
      "Epoch 898. loss = 0.034470\n",
      "Epoch 899. loss = 0.034423\n",
      "Epoch 900. loss = 0.034376\n",
      "Epoch 901. loss = 0.034330\n",
      "Epoch 902. loss = 0.034283\n",
      "Epoch 903. loss = 0.034237\n",
      "Epoch 904. loss = 0.034191\n",
      "Epoch 905. loss = 0.034145\n",
      "Epoch 906. loss = 0.034099\n",
      "Epoch 907. loss = 0.034053\n",
      "Epoch 908. loss = 0.034007\n",
      "Epoch 909. loss = 0.033962\n",
      "Epoch 910. loss = 0.033916\n",
      "Epoch 911. loss = 0.033871\n",
      "Epoch 912. loss = 0.033826\n",
      "Epoch 913. loss = 0.033781\n",
      "Epoch 914. loss = 0.033736\n",
      "Epoch 915. loss = 0.033691\n",
      "Epoch 916. loss = 0.033646\n",
      "Epoch 917. loss = 0.033602\n",
      "Epoch 918. loss = 0.033557\n",
      "Epoch 919. loss = 0.033513\n",
      "Epoch 920. loss = 0.033468\n",
      "Epoch 921. loss = 0.033424\n",
      "Epoch 922. loss = 0.033380\n",
      "Epoch 923. loss = 0.033336\n",
      "Epoch 924. loss = 0.033293\n",
      "Epoch 925. loss = 0.033249\n",
      "Epoch 926. loss = 0.033205\n",
      "Epoch 927. loss = 0.033162\n",
      "Epoch 928. loss = 0.033118\n",
      "Epoch 929. loss = 0.033075\n",
      "Epoch 930. loss = 0.033032\n",
      "Epoch 931. loss = 0.032989\n",
      "Epoch 932. loss = 0.032946\n",
      "Epoch 933. loss = 0.032903\n",
      "Epoch 934. loss = 0.032861\n",
      "Epoch 935. loss = 0.032818\n",
      "Epoch 936. loss = 0.032776\n",
      "Epoch 937. loss = 0.032733\n",
      "Epoch 938. loss = 0.032691\n",
      "Epoch 939. loss = 0.032649\n",
      "Epoch 940. loss = 0.032607\n",
      "Epoch 941. loss = 0.032565\n",
      "Epoch 942. loss = 0.032523\n",
      "Epoch 943. loss = 0.032481\n",
      "Epoch 944. loss = 0.032440\n",
      "Epoch 945. loss = 0.032398\n",
      "Epoch 946. loss = 0.032357\n",
      "Epoch 947. loss = 0.032316\n",
      "Epoch 948. loss = 0.032274\n",
      "Epoch 949. loss = 0.032233\n",
      "Epoch 950. loss = 0.032192\n",
      "Epoch 951. loss = 0.032151\n",
      "Epoch 952. loss = 0.032111\n",
      "Epoch 953. loss = 0.032070\n",
      "Epoch 954. loss = 0.032029\n",
      "Epoch 955. loss = 0.031989\n",
      "Epoch 956. loss = 0.031949\n",
      "Epoch 957. loss = 0.031908\n",
      "Epoch 958. loss = 0.031868\n",
      "Epoch 959. loss = 0.031828\n",
      "Epoch 960. loss = 0.031788\n",
      "Epoch 961. loss = 0.031748\n",
      "Epoch 962. loss = 0.031708\n",
      "Epoch 963. loss = 0.031669\n",
      "Epoch 964. loss = 0.031629\n",
      "Epoch 965. loss = 0.031590\n",
      "Epoch 966. loss = 0.031550\n",
      "Epoch 967. loss = 0.031511\n",
      "Epoch 968. loss = 0.031472\n",
      "Epoch 969. loss = 0.031433\n",
      "Epoch 970. loss = 0.031394\n",
      "Epoch 971. loss = 0.031355\n",
      "Epoch 972. loss = 0.031316\n",
      "Epoch 973. loss = 0.031277\n",
      "Epoch 974. loss = 0.031239\n",
      "Epoch 975. loss = 0.031200\n",
      "Epoch 976. loss = 0.031162\n",
      "Epoch 977. loss = 0.031124\n",
      "Epoch 978. loss = 0.031085\n",
      "Epoch 979. loss = 0.031047\n",
      "Epoch 980. loss = 0.031009\n",
      "Epoch 981. loss = 0.030971\n",
      "Epoch 982. loss = 0.030933\n",
      "Epoch 983. loss = 0.030895\n",
      "Epoch 984. loss = 0.030858\n",
      "Epoch 985. loss = 0.030820\n",
      "Epoch 986. loss = 0.030783\n",
      "Epoch 987. loss = 0.030745\n",
      "Epoch 988. loss = 0.030708\n",
      "Epoch 989. loss = 0.030671\n",
      "Epoch 990. loss = 0.030634\n",
      "Epoch 991. loss = 0.030597\n",
      "Epoch 992. loss = 0.030560\n",
      "Epoch 993. loss = 0.030523\n",
      "Epoch 994. loss = 0.030486\n",
      "Epoch 995. loss = 0.030449\n",
      "Epoch 996. loss = 0.030413\n",
      "Epoch 997. loss = 0.030376\n",
      "Epoch 998. loss = 0.030340\n",
      "Epoch 999. loss = 0.030303\n",
      "0.9908039569854736\n"
     ]
    }
   ],
   "source": [
    "# XOR problem\n",
    "\n",
    "import dynet as dy\n",
    "import random\n",
    "\n",
    "data = [([0,1],0),\n",
    "        ([1,0],0),\n",
    "        ([0,0],1),\n",
    "        ([1,1],1)]\n",
    "\n",
    "model = dy.Model()\n",
    "pU = model.add_parameters((4,2))\n",
    "pb = model.add_parameters(4)\n",
    "pv = model.add_parameters(4)\n",
    "\n",
    "trainer = dy.SimpleSGDTrainer(model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EPOCHS = 1000\n",
    "\n",
    "for epoch in range (EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    for x,y in data:\n",
    "        dy.renew_cg()\n",
    "        U = dy.parameter(pU)\n",
    "        b = dy.parameter(pb)\n",
    "        v = dy.parameter(pv)\n",
    "        x = dy.inputVector(x)\n",
    "        \n",
    "        # prediction\n",
    "        \n",
    "        yhat = dy.logistic(dy.dot_product(v,dy.tanh(U * x + b)))\n",
    "        \n",
    "        # calculate loss\n",
    "        if y == 0:\n",
    "            loss = -dy.log(1 - yhat)\n",
    "        else:\n",
    "            loss = -dy.log(yhat)\n",
    "            \n",
    "        epoch_loss += loss.scalar_value()\n",
    "        \n",
    "        loss.backward()\n",
    "        trainer.update()\n",
    "        \n",
    "    print(\"Epoch %d. loss = %f\" % (epoch, epoch_loss))\n",
    "    \n",
    "inputData = [0,0]\n",
    "i = dy.inputVector(inputData)\n",
    "yhat = dy.logistic(dy.dot_product(v,dy.tanh(U * x + b)))\n",
    "\n",
    "print(yhat.value())\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset...\n",
      "Loading word vectors...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'A'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-5027def10f37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mdy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrenew_cg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdo_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-5027def10f37>\u001b[0m in \u001b[0;36mpredict_labels\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencode_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mh\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mlayer1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-5027def10f37>\u001b[0m in \u001b[0;36mencode_doc\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mencode_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw2i\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0membs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mE\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-5027def10f37>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mencode_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw2i\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0membs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mE\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'A'"
     ]
    }
   ],
   "source": [
    "# Deep Averaging Network (Look-up table)\n",
    "\n",
    "import dynet as dy\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "\n",
    "l2i = {}\n",
    "i2l = {}\n",
    "\n",
    "V = 1000\n",
    "HID = 20\n",
    "EDIM = 50\n",
    "NOUT = 5\n",
    "\n",
    "\n",
    "def predict_labels(doc):\n",
    "    x = encode_doc(doc)\n",
    "    h= layer1(x)\n",
    "    y = layer2(h)\n",
    "    \n",
    "    return dy.softmax(y)\n",
    "\n",
    "\n",
    "def layer1(x):\n",
    "    W = dy.parameter(pW1)\n",
    "    b = dy.parameter(pb1)\n",
    "    \n",
    "    return dy.tanh(W * x + b)\n",
    "\n",
    "def layer2(x):\n",
    "    W = dy.parameter(pW2)\n",
    "    b = dy.parameter(pb2)\n",
    "    \n",
    "    return dy.tanh(W * x + b)\n",
    "\n",
    "def encode_doc(doc):\n",
    "    doc = [w2i[w] for w in doc]\n",
    "    embs = [E[idx] for idx in doc]\n",
    "    \n",
    "    return dy.esum(embs)\n",
    "\n",
    "def do_loss(probs, label):\n",
    "    label = l2i[label]\n",
    "    \n",
    "    return -dy.log(dy.pick(probs,label))\n",
    "\n",
    "def classify(doc):\n",
    "    dy.renew_cg()\n",
    "    probs = predict_labels(doc)\n",
    "    \n",
    "    vals = probs.npvalue()\n",
    "    \n",
    "    return i2l[np.argmax(vals)]\n",
    "\n",
    "def read_dataset(fname):\n",
    "    print('Reading dataset...')\n",
    "    df = pd.read_csv(fname)\n",
    "\n",
    "    tags_index = {\n",
    "        \"sci-fi\": 1,\n",
    "        \"action\": 2,\n",
    "        \"comedy\": 3,\n",
    "        \"fantasy\": 4,\n",
    "        \"animation\": 5,\n",
    "        \"romance\": 6,\n",
    "    }\n",
    "    \n",
    "    index_tags = {\n",
    "        1 : \"sci-fi\",\n",
    "        2 : \"action\",\n",
    "        3 : \"comedy\",\n",
    "        4 : \"fantasy\",\n",
    "        5 : \"animation\",\n",
    "        6 : \"romance\",\n",
    "    }\n",
    "   \n",
    "    data = []\n",
    "    with open(fname) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if line_count == 0:\n",
    "                line_count += 1\n",
    "            else:\n",
    "                data.append([row[2],tags_index[row[3].strip(\"\\n\")]])\n",
    "                line_count += 1\n",
    "\n",
    "    return data, tags_index,index_tags\n",
    "\n",
    "def read_glove():\n",
    "    print('Loading word vectors...')\n",
    "    word2vec = {}\n",
    "    embedding = []\n",
    "    embeds = []\n",
    "    word2idx = {}\n",
    "    with open('glove.6B.50d.txt', encoding='utf-8') as f:\n",
    "      for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        word2idx[word] = len(embeds)\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "        embeds.append(vec)\n",
    "    return np.array(embeds),word2idx\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "data, l2i, i2l = read_dataset(\"tagged_plots_movielens.csv\")\n",
    "embedding , w2i = read_glove()\n",
    "\n",
    "pW1 = model.add_parameters((HID, EDIM))\n",
    "pb1 = model.add_parameters(HID)\n",
    "pW2 = model.add_parameters((NOUT, HID))\n",
    "pb2 = model.add_parameters(NOUT)\n",
    "E = model.add_lookup_parameters((len(w2idx), EDIM),init =  embedding )\n",
    "\n",
    "for (doc, label) in data:\n",
    "    dy.renew_cg()\n",
    "    probs = predict_labels(doc)\n",
    "    \n",
    "    loss = do_loss(probs,label)\n",
    "    loss.forward()\n",
    "    loss.backward()\n",
    "    trainer.update()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
