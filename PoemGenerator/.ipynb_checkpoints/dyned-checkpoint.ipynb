{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation Graph and Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expression 5/1\n",
      "[ 1.  2.  3.  4.  5.  6.  7.  8.  6.  8. 10. 12.  2.  3.  4.  5.]\n"
     ]
    }
   ],
   "source": [
    "import dynet as dy\n",
    "\n",
    "dy.renew_cg()\n",
    "\n",
    "v1 = dy.inputVector([1,2,3,4])\n",
    "\n",
    "v2 = dy.inputVector([5,6,7,8])\n",
    "\n",
    "v3 = v1 + v2\n",
    "\n",
    "v4 = v3 * 2\n",
    "\n",
    "v5 = v1 + 1\n",
    "\n",
    "v6 = dy.concatenate([v1,v2,v3,v5])\n",
    "\n",
    "print(v6)\n",
    "\n",
    "print(v6.npvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dy.parameter(...) call is now DEPRECATED.\n",
      "        There is no longer need to explicitly add parameters to the computation graph.\n",
      "        Any used parameter will be added automatically.\n",
      "expression 4/2\n",
      "[-1.77732956 -2.50622225 -0.1935235  -2.13071227  0.45663989 -0.12065721\n",
      "  0.6087963  -2.34236336 -1.32552719  0.15933138 -1.0123167  -1.16767502\n",
      " -0.15759878  1.14957309  0.28480363  0.49910116 -0.48600975  0.01581281\n",
      " -0.15707356 -1.54209113]\n"
     ]
    }
   ],
   "source": [
    "model = dy.Model()\n",
    "\n",
    "pW = model.add_parameters((20,4))\n",
    "pb = model.add_parameters(20)\n",
    "\n",
    "dy.renew_cg()\n",
    "\n",
    "x = dy.inputVector([1,2,3,4])\n",
    "W = dy.parameter(pW)\n",
    "b = dy.parameter(pb)\n",
    "\n",
    "y = W * x + b\n",
    "\n",
    "print(y)\n",
    "\n",
    "print(y.npvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02104897]\n"
     ]
    }
   ],
   "source": [
    "model = dy.Model()\n",
    "\n",
    "trainer = dy.SimpleSGDTrainer(model)\n",
    "\n",
    "p_v = model.add_parameters(10)\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range (EPOCHS):\n",
    "    dy.renew_cg()\n",
    "    \n",
    "    v = dy.parameter(p_v)\n",
    "    v2 = dy.dot_product(v,v)\n",
    "    v2.forward()\n",
    "    \n",
    "    v2.backward()\n",
    "    \n",
    "    trainer.update()\n",
    "    \n",
    "print(v2.npvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. loss = 2.995412\n",
      "Epoch 1. loss = 2.944337\n",
      "Epoch 2. loss = 2.907920\n",
      "Epoch 3. loss = 2.881539\n",
      "Epoch 4. loss = 2.862026\n",
      "Epoch 5. loss = 2.847221\n",
      "Epoch 6. loss = 2.835645\n",
      "Epoch 7. loss = 2.826287\n",
      "Epoch 8. loss = 2.818450\n",
      "Epoch 9. loss = 2.811653\n",
      "Epoch 10. loss = 2.805559\n",
      "Epoch 11. loss = 2.799932\n",
      "Epoch 12. loss = 2.794607\n",
      "Epoch 13. loss = 2.789462\n",
      "Epoch 14. loss = 2.784411\n",
      "Epoch 15. loss = 2.779390\n",
      "Epoch 16. loss = 2.774351\n",
      "Epoch 17. loss = 2.769260\n",
      "Epoch 18. loss = 2.764088\n",
      "Epoch 19. loss = 2.758814\n",
      "Epoch 20. loss = 2.753422\n",
      "Epoch 21. loss = 2.747898\n",
      "Epoch 22. loss = 2.742233\n",
      "Epoch 23. loss = 2.736417\n",
      "Epoch 24. loss = 2.730443\n",
      "Epoch 25. loss = 2.724306\n",
      "Epoch 26. loss = 2.718000\n",
      "Epoch 27. loss = 2.711523\n",
      "Epoch 28. loss = 2.704870\n",
      "Epoch 29. loss = 2.698039\n",
      "Epoch 30. loss = 2.691027\n",
      "Epoch 31. loss = 2.683833\n",
      "Epoch 32. loss = 2.676454\n",
      "Epoch 33. loss = 2.668891\n",
      "Epoch 34. loss = 2.661141\n",
      "Epoch 35. loss = 2.653205\n",
      "Epoch 36. loss = 2.645081\n",
      "Epoch 37. loss = 2.636769\n",
      "Epoch 38. loss = 2.628271\n",
      "Epoch 39. loss = 2.619586\n",
      "Epoch 40. loss = 2.610716\n",
      "Epoch 41. loss = 2.601663\n",
      "Epoch 42. loss = 2.592428\n",
      "Epoch 43. loss = 2.583015\n",
      "Epoch 44. loss = 2.573426\n",
      "Epoch 45. loss = 2.563665\n",
      "Epoch 46. loss = 2.553737\n",
      "Epoch 47. loss = 2.543646\n",
      "Epoch 48. loss = 2.533397\n",
      "Epoch 49. loss = 2.522998\n",
      "Epoch 50. loss = 2.512454\n",
      "Epoch 51. loss = 2.501772\n",
      "Epoch 52. loss = 2.490959\n",
      "Epoch 53. loss = 2.480024\n",
      "Epoch 54. loss = 2.468974\n",
      "Epoch 55. loss = 2.457817\n",
      "Epoch 56. loss = 2.446562\n",
      "Epoch 57. loss = 2.435217\n",
      "Epoch 58. loss = 2.423791\n",
      "Epoch 59. loss = 2.412291\n",
      "Epoch 60. loss = 2.400728\n",
      "Epoch 61. loss = 2.389107\n",
      "Epoch 62. loss = 2.377439\n",
      "Epoch 63. loss = 2.365731\n",
      "Epoch 64. loss = 2.353990\n",
      "Epoch 65. loss = 2.342224\n",
      "Epoch 66. loss = 2.330440\n",
      "Epoch 67. loss = 2.318645\n",
      "Epoch 68. loss = 2.306846\n",
      "Epoch 69. loss = 2.295049\n",
      "Epoch 70. loss = 2.283260\n",
      "Epoch 71. loss = 2.271485\n",
      "Epoch 72. loss = 2.259728\n",
      "Epoch 73. loss = 2.247996\n",
      "Epoch 74. loss = 2.236293\n",
      "Epoch 75. loss = 2.224623\n",
      "Epoch 76. loss = 2.212989\n",
      "Epoch 77. loss = 2.201397\n",
      "Epoch 78. loss = 2.189849\n",
      "Epoch 79. loss = 2.178348\n",
      "Epoch 80. loss = 2.166896\n",
      "Epoch 81. loss = 2.155496\n",
      "Epoch 82. loss = 2.144150\n",
      "Epoch 83. loss = 2.132858\n",
      "Epoch 84. loss = 2.121623\n",
      "Epoch 85. loss = 2.110444\n",
      "Epoch 86. loss = 2.099321\n",
      "Epoch 87. loss = 2.088255\n",
      "Epoch 88. loss = 2.077245\n",
      "Epoch 89. loss = 2.066288\n",
      "Epoch 90. loss = 2.055384\n",
      "Epoch 91. loss = 2.044530\n",
      "Epoch 92. loss = 2.033724\n",
      "Epoch 93. loss = 2.022962\n",
      "Epoch 94. loss = 2.012241\n",
      "Epoch 95. loss = 2.001555\n",
      "Epoch 96. loss = 1.990901\n",
      "Epoch 97. loss = 1.980273\n",
      "Epoch 98. loss = 1.969664\n",
      "Epoch 99. loss = 1.959069\n",
      "Epoch 100. loss = 1.948478\n",
      "Epoch 101. loss = 1.937885\n",
      "Epoch 102. loss = 1.927282\n",
      "Epoch 103. loss = 1.916658\n",
      "Epoch 104. loss = 1.906003\n",
      "Epoch 105. loss = 1.895308\n",
      "Epoch 106. loss = 1.884562\n",
      "Epoch 107. loss = 1.873751\n",
      "Epoch 108. loss = 1.862863\n",
      "Epoch 109. loss = 1.851886\n",
      "Epoch 110. loss = 1.840805\n",
      "Epoch 111. loss = 1.829606\n",
      "Epoch 112. loss = 1.818274\n",
      "Epoch 113. loss = 1.806794\n",
      "Epoch 114. loss = 1.795150\n",
      "Epoch 115. loss = 1.783326\n",
      "Epoch 116. loss = 1.771305\n",
      "Epoch 117. loss = 1.759071\n",
      "Epoch 118. loss = 1.746609\n",
      "Epoch 119. loss = 1.733901\n",
      "Epoch 120. loss = 1.720933\n",
      "Epoch 121. loss = 1.707689\n",
      "Epoch 122. loss = 1.694156\n",
      "Epoch 123. loss = 1.680319\n",
      "Epoch 124. loss = 1.666167\n",
      "Epoch 125. loss = 1.651691\n",
      "Epoch 126. loss = 1.636880\n",
      "Epoch 127. loss = 1.621729\n",
      "Epoch 128. loss = 1.606233\n",
      "Epoch 129. loss = 1.590389\n",
      "Epoch 130. loss = 1.574198\n",
      "Epoch 131. loss = 1.557662\n",
      "Epoch 132. loss = 1.540788\n",
      "Epoch 133. loss = 1.523583\n",
      "Epoch 134. loss = 1.506058\n",
      "Epoch 135. loss = 1.488227\n",
      "Epoch 136. loss = 1.470108\n",
      "Epoch 137. loss = 1.451717\n",
      "Epoch 138. loss = 1.433078\n",
      "Epoch 139. loss = 1.414214\n",
      "Epoch 140. loss = 1.395151\n",
      "Epoch 141. loss = 1.375914\n",
      "Epoch 142. loss = 1.356534\n",
      "Epoch 143. loss = 1.337039\n",
      "Epoch 144. loss = 1.317460\n",
      "Epoch 145. loss = 1.297827\n",
      "Epoch 146. loss = 1.278172\n",
      "Epoch 147. loss = 1.258524\n",
      "Epoch 148. loss = 1.238914\n",
      "Epoch 149. loss = 1.219370\n",
      "Epoch 150. loss = 1.199918\n",
      "Epoch 151. loss = 1.180588\n",
      "Epoch 152. loss = 1.161401\n",
      "Epoch 153. loss = 1.142383\n",
      "Epoch 154. loss = 1.123554\n",
      "Epoch 155. loss = 1.104933\n",
      "Epoch 156. loss = 1.086538\n",
      "Epoch 157. loss = 1.068384\n",
      "Epoch 158. loss = 1.050486\n",
      "Epoch 159. loss = 1.032854\n",
      "Epoch 160. loss = 1.015499\n",
      "Epoch 161. loss = 0.998429\n",
      "Epoch 162. loss = 0.981651\n",
      "Epoch 163. loss = 0.965170\n",
      "Epoch 164. loss = 0.948990\n",
      "Epoch 165. loss = 0.933114\n",
      "Epoch 166. loss = 0.917542\n",
      "Epoch 167. loss = 0.902276\n",
      "Epoch 168. loss = 0.887315\n",
      "Epoch 169. loss = 0.872656\n",
      "Epoch 170. loss = 0.858300\n",
      "Epoch 171. loss = 0.844241\n",
      "Epoch 172. loss = 0.830479\n",
      "Epoch 173. loss = 0.817007\n",
      "Epoch 174. loss = 0.803824\n",
      "Epoch 175. loss = 0.790923\n",
      "Epoch 176. loss = 0.778301\n",
      "Epoch 177. loss = 0.765952\n",
      "Epoch 178. loss = 0.753872\n",
      "Epoch 179. loss = 0.742055\n",
      "Epoch 180. loss = 0.730495\n",
      "Epoch 181. loss = 0.719188\n",
      "Epoch 182. loss = 0.708129\n",
      "Epoch 183. loss = 0.697311\n",
      "Epoch 184. loss = 0.686728\n",
      "Epoch 185. loss = 0.676378\n",
      "Epoch 186. loss = 0.666252\n",
      "Epoch 187. loss = 0.656347\n",
      "Epoch 188. loss = 0.646656\n",
      "Epoch 189. loss = 0.637176\n",
      "Epoch 190. loss = 0.627901\n",
      "Epoch 191. loss = 0.618825\n",
      "Epoch 192. loss = 0.609944\n",
      "Epoch 193. loss = 0.601254\n",
      "Epoch 194. loss = 0.592749\n",
      "Epoch 195. loss = 0.584425\n",
      "Epoch 196. loss = 0.576278\n",
      "Epoch 197. loss = 0.568303\n",
      "Epoch 198. loss = 0.560495\n",
      "Epoch 199. loss = 0.552851\n",
      "Epoch 200. loss = 0.545367\n",
      "Epoch 201. loss = 0.538038\n",
      "Epoch 202. loss = 0.530861\n",
      "Epoch 203. loss = 0.523832\n",
      "Epoch 204. loss = 0.516948\n",
      "Epoch 205. loss = 0.510204\n",
      "Epoch 206. loss = 0.503597\n",
      "Epoch 207. loss = 0.497124\n",
      "Epoch 208. loss = 0.490781\n",
      "Epoch 209. loss = 0.484566\n",
      "Epoch 210. loss = 0.478474\n",
      "Epoch 211. loss = 0.472504\n",
      "Epoch 212. loss = 0.466651\n",
      "Epoch 213. loss = 0.460915\n",
      "Epoch 214. loss = 0.455290\n",
      "Epoch 215. loss = 0.449775\n",
      "Epoch 216. loss = 0.444367\n",
      "Epoch 217. loss = 0.439063\n",
      "Epoch 218. loss = 0.433861\n",
      "Epoch 219. loss = 0.428758\n",
      "Epoch 220. loss = 0.423753\n",
      "Epoch 221. loss = 0.418842\n",
      "Epoch 222. loss = 0.414023\n",
      "Epoch 223. loss = 0.409295\n",
      "Epoch 224. loss = 0.404655\n",
      "Epoch 225. loss = 0.400100\n",
      "Epoch 226. loss = 0.395630\n",
      "Epoch 227. loss = 0.391241\n",
      "Epoch 228. loss = 0.386932\n",
      "Epoch 229. loss = 0.382702\n",
      "Epoch 230. loss = 0.378548\n",
      "Epoch 231. loss = 0.374469\n",
      "Epoch 232. loss = 0.370462\n",
      "Epoch 233. loss = 0.366526\n",
      "Epoch 234. loss = 0.362660\n",
      "Epoch 235. loss = 0.358862\n",
      "Epoch 236. loss = 0.355131\n",
      "Epoch 237. loss = 0.351464\n",
      "Epoch 238. loss = 0.347860\n",
      "Epoch 239. loss = 0.344319\n",
      "Epoch 240. loss = 0.340838\n",
      "Epoch 241. loss = 0.337416\n",
      "Epoch 242. loss = 0.334053\n",
      "Epoch 243. loss = 0.330745\n",
      "Epoch 244. loss = 0.327494\n",
      "Epoch 245. loss = 0.324296\n",
      "Epoch 246. loss = 0.321151\n",
      "Epoch 247. loss = 0.318059\n",
      "Epoch 248. loss = 0.315017\n",
      "Epoch 249. loss = 0.312024\n",
      "Epoch 250. loss = 0.309081\n",
      "Epoch 251. loss = 0.306184\n",
      "Epoch 252. loss = 0.303334\n",
      "Epoch 253. loss = 0.300529\n",
      "Epoch 254. loss = 0.297769\n",
      "Epoch 255. loss = 0.295053\n",
      "Epoch 256. loss = 0.292379\n",
      "Epoch 257. loss = 0.289747\n",
      "Epoch 258. loss = 0.287155\n",
      "Epoch 259. loss = 0.284604\n",
      "Epoch 260. loss = 0.282092\n",
      "Epoch 261. loss = 0.279619\n",
      "Epoch 262. loss = 0.277183\n",
      "Epoch 263. loss = 0.274784\n",
      "Epoch 264. loss = 0.272421\n",
      "Epoch 265. loss = 0.270093\n",
      "Epoch 266. loss = 0.267800\n",
      "Epoch 267. loss = 0.265541\n",
      "Epoch 268. loss = 0.263315\n",
      "Epoch 269. loss = 0.261122\n",
      "Epoch 270. loss = 0.258961\n",
      "Epoch 271. loss = 0.256831\n",
      "Epoch 272. loss = 0.254732\n",
      "Epoch 273. loss = 0.252663\n",
      "Epoch 274. loss = 0.250623\n",
      "Epoch 275. loss = 0.248613\n",
      "Epoch 276. loss = 0.246631\n",
      "Epoch 277. loss = 0.244676\n",
      "Epoch 278. loss = 0.242750\n",
      "Epoch 279. loss = 0.240849\n",
      "Epoch 280. loss = 0.238976\n",
      "Epoch 281. loss = 0.237127\n",
      "Epoch 282. loss = 0.235305\n",
      "Epoch 283. loss = 0.233507\n",
      "Epoch 284. loss = 0.231733\n",
      "Epoch 285. loss = 0.229984\n",
      "Epoch 286. loss = 0.228257\n",
      "Epoch 287. loss = 0.226554\n",
      "Epoch 288. loss = 0.224874\n",
      "Epoch 289. loss = 0.223216\n",
      "Epoch 290. loss = 0.221579\n",
      "Epoch 291. loss = 0.219964\n",
      "Epoch 292. loss = 0.218370\n",
      "Epoch 293. loss = 0.216797\n",
      "Epoch 294. loss = 0.215243\n",
      "Epoch 295. loss = 0.213710\n",
      "Epoch 296. loss = 0.212196\n",
      "Epoch 297. loss = 0.210702\n",
      "Epoch 298. loss = 0.209226\n",
      "Epoch 299. loss = 0.207769\n",
      "Epoch 300. loss = 0.206331\n",
      "Epoch 301. loss = 0.204910\n",
      "Epoch 302. loss = 0.203506\n",
      "Epoch 303. loss = 0.202121\n",
      "Epoch 304. loss = 0.200751\n",
      "Epoch 305. loss = 0.199399\n",
      "Epoch 306. loss = 0.198063\n",
      "Epoch 307. loss = 0.196744\n",
      "Epoch 308. loss = 0.195440\n",
      "Epoch 309. loss = 0.194152\n",
      "Epoch 310. loss = 0.192879\n",
      "Epoch 311. loss = 0.191621\n",
      "Epoch 312. loss = 0.190378\n",
      "Epoch 313. loss = 0.189150\n",
      "Epoch 314. loss = 0.187936\n",
      "Epoch 315. loss = 0.186736\n",
      "Epoch 316. loss = 0.185550\n",
      "Epoch 317. loss = 0.184378\n",
      "Epoch 318. loss = 0.183219\n",
      "Epoch 319. loss = 0.182073\n",
      "Epoch 320. loss = 0.180940\n",
      "Epoch 321. loss = 0.179821\n",
      "Epoch 322. loss = 0.178713\n",
      "Epoch 323. loss = 0.177619\n",
      "Epoch 324. loss = 0.176536\n",
      "Epoch 325. loss = 0.175466\n",
      "Epoch 326. loss = 0.174407\n",
      "Epoch 327. loss = 0.173360\n",
      "Epoch 328. loss = 0.172324\n",
      "Epoch 329. loss = 0.171300\n",
      "Epoch 330. loss = 0.170287\n",
      "Epoch 331. loss = 0.169284\n",
      "Epoch 332. loss = 0.168293\n",
      "Epoch 333. loss = 0.167312\n",
      "Epoch 334. loss = 0.166342\n",
      "Epoch 335. loss = 0.165381\n",
      "Epoch 336. loss = 0.164431\n",
      "Epoch 337. loss = 0.163492\n",
      "Epoch 338. loss = 0.162561\n",
      "Epoch 339. loss = 0.161641\n",
      "Epoch 340. loss = 0.160730\n",
      "Epoch 341. loss = 0.159829\n",
      "Epoch 342. loss = 0.158937\n",
      "Epoch 343. loss = 0.158053\n",
      "Epoch 344. loss = 0.157179\n",
      "Epoch 345. loss = 0.156314\n",
      "Epoch 346. loss = 0.155458\n",
      "Epoch 347. loss = 0.154610\n",
      "Epoch 348. loss = 0.153771\n",
      "Epoch 349. loss = 0.152940\n",
      "Epoch 350. loss = 0.152117\n",
      "Epoch 351. loss = 0.151303\n",
      "Epoch 352. loss = 0.150496\n",
      "Epoch 353. loss = 0.149697\n",
      "Epoch 354. loss = 0.148907\n",
      "Epoch 355. loss = 0.148124\n",
      "Epoch 356. loss = 0.147348\n",
      "Epoch 357. loss = 0.146580\n",
      "Epoch 358. loss = 0.145819\n",
      "Epoch 359. loss = 0.145066\n",
      "Epoch 360. loss = 0.144320\n",
      "Epoch 361. loss = 0.143581\n",
      "Epoch 362. loss = 0.142849\n",
      "Epoch 363. loss = 0.142123\n",
      "Epoch 364. loss = 0.141405\n",
      "Epoch 365. loss = 0.140693\n",
      "Epoch 366. loss = 0.139988\n",
      "Epoch 367. loss = 0.139289\n",
      "Epoch 368. loss = 0.138597\n",
      "Epoch 369. loss = 0.137911\n",
      "Epoch 370. loss = 0.137232\n",
      "Epoch 371. loss = 0.136559\n",
      "Epoch 372. loss = 0.135891\n",
      "Epoch 373. loss = 0.135230\n",
      "Epoch 374. loss = 0.134575\n",
      "Epoch 375. loss = 0.133926\n",
      "Epoch 376. loss = 0.133282\n",
      "Epoch 377. loss = 0.132644\n",
      "Epoch 378. loss = 0.132012\n",
      "Epoch 379. loss = 0.131386\n",
      "Epoch 380. loss = 0.130764\n",
      "Epoch 381. loss = 0.130149\n",
      "Epoch 382. loss = 0.129538\n",
      "Epoch 383. loss = 0.128933\n",
      "Epoch 384. loss = 0.128334\n",
      "Epoch 385. loss = 0.127739\n",
      "Epoch 386. loss = 0.127150\n",
      "Epoch 387. loss = 0.126565\n",
      "Epoch 388. loss = 0.125986\n",
      "Epoch 389. loss = 0.125411\n",
      "Epoch 390. loss = 0.124842\n",
      "Epoch 391. loss = 0.124277\n",
      "Epoch 392. loss = 0.123717\n",
      "Epoch 393. loss = 0.123161\n",
      "Epoch 394. loss = 0.122610\n",
      "Epoch 395. loss = 0.122064\n",
      "Epoch 396. loss = 0.121523\n",
      "Epoch 397. loss = 0.120985\n",
      "Epoch 398. loss = 0.120452\n",
      "Epoch 399. loss = 0.119924\n",
      "Epoch 400. loss = 0.119400\n",
      "Epoch 401. loss = 0.118880\n",
      "Epoch 402. loss = 0.118364\n",
      "Epoch 403. loss = 0.117853\n",
      "Epoch 404. loss = 0.117345\n",
      "Epoch 405. loss = 0.116842\n",
      "Epoch 406. loss = 0.116342\n",
      "Epoch 407. loss = 0.115847\n",
      "Epoch 408. loss = 0.115355\n",
      "Epoch 409. loss = 0.114868\n",
      "Epoch 410. loss = 0.114384\n",
      "Epoch 411. loss = 0.113904\n",
      "Epoch 412. loss = 0.113428\n",
      "Epoch 413. loss = 0.112955\n",
      "Epoch 414. loss = 0.112487\n",
      "Epoch 415. loss = 0.112021\n",
      "Epoch 416. loss = 0.111560\n",
      "Epoch 417. loss = 0.111101\n",
      "Epoch 418. loss = 0.110647\n",
      "Epoch 419. loss = 0.110196\n",
      "Epoch 420. loss = 0.109748\n",
      "Epoch 421. loss = 0.109304\n",
      "Epoch 422. loss = 0.108863\n",
      "Epoch 423. loss = 0.108425\n",
      "Epoch 424. loss = 0.107991\n",
      "Epoch 425. loss = 0.107559\n",
      "Epoch 426. loss = 0.107131\n",
      "Epoch 427. loss = 0.106707\n",
      "Epoch 428. loss = 0.106285\n",
      "Epoch 429. loss = 0.105866\n",
      "Epoch 430. loss = 0.105451\n",
      "Epoch 431. loss = 0.105038\n",
      "Epoch 432. loss = 0.104629\n",
      "Epoch 433. loss = 0.104222\n",
      "Epoch 434. loss = 0.103819\n",
      "Epoch 435. loss = 0.103418\n",
      "Epoch 436. loss = 0.103020\n",
      "Epoch 437. loss = 0.102626\n",
      "Epoch 438. loss = 0.102234\n",
      "Epoch 439. loss = 0.101844\n",
      "Epoch 440. loss = 0.101458\n",
      "Epoch 441. loss = 0.101074\n",
      "Epoch 442. loss = 0.100693\n",
      "Epoch 443. loss = 0.100314\n",
      "Epoch 444. loss = 0.099939\n",
      "Epoch 445. loss = 0.099566\n",
      "Epoch 446. loss = 0.099195\n",
      "Epoch 447. loss = 0.098827\n",
      "Epoch 448. loss = 0.098461\n",
      "Epoch 449. loss = 0.098099\n",
      "Epoch 450. loss = 0.097738\n",
      "Epoch 451. loss = 0.097380\n",
      "Epoch 452. loss = 0.097025\n",
      "Epoch 453. loss = 0.096672\n",
      "Epoch 454. loss = 0.096321\n",
      "Epoch 455. loss = 0.095973\n",
      "Epoch 456. loss = 0.095627\n",
      "Epoch 457. loss = 0.095283\n",
      "Epoch 458. loss = 0.094942\n",
      "Epoch 459. loss = 0.094602\n",
      "Epoch 460. loss = 0.094266\n",
      "Epoch 461. loss = 0.093931\n",
      "Epoch 462. loss = 0.093599\n",
      "Epoch 463. loss = 0.093269\n",
      "Epoch 464. loss = 0.092941\n",
      "Epoch 465. loss = 0.092615\n",
      "Epoch 466. loss = 0.092291\n",
      "Epoch 467. loss = 0.091969\n",
      "Epoch 468. loss = 0.091650\n",
      "Epoch 469. loss = 0.091332\n",
      "Epoch 470. loss = 0.091017\n",
      "Epoch 471. loss = 0.090704\n",
      "Epoch 472. loss = 0.090392\n",
      "Epoch 473. loss = 0.090083\n",
      "Epoch 474. loss = 0.089775\n",
      "Epoch 475. loss = 0.089470\n",
      "Epoch 476. loss = 0.089166\n",
      "Epoch 477. loss = 0.088865\n",
      "Epoch 478. loss = 0.088565\n",
      "Epoch 479. loss = 0.088267\n",
      "Epoch 480. loss = 0.087972\n",
      "Epoch 481. loss = 0.087677\n",
      "Epoch 482. loss = 0.087385\n",
      "Epoch 483. loss = 0.087095\n",
      "Epoch 484. loss = 0.086806\n",
      "Epoch 485. loss = 0.086519\n",
      "Epoch 486. loss = 0.086234\n",
      "Epoch 487. loss = 0.085951\n",
      "Epoch 488. loss = 0.085669\n",
      "Epoch 489. loss = 0.085389\n",
      "Epoch 490. loss = 0.085111\n",
      "Epoch 491. loss = 0.084834\n",
      "Epoch 492. loss = 0.084560\n",
      "Epoch 493. loss = 0.084286\n",
      "Epoch 494. loss = 0.084015\n",
      "Epoch 495. loss = 0.083745\n",
      "Epoch 496. loss = 0.083477\n",
      "Epoch 497. loss = 0.083210\n",
      "Epoch 498. loss = 0.082945\n",
      "Epoch 499. loss = 0.082681\n",
      "Epoch 500. loss = 0.082420\n",
      "Epoch 501. loss = 0.082159\n",
      "Epoch 502. loss = 0.081900\n",
      "Epoch 503. loss = 0.081643\n",
      "Epoch 504. loss = 0.081387\n",
      "Epoch 505. loss = 0.081133\n",
      "Epoch 506. loss = 0.080880\n",
      "Epoch 507. loss = 0.080628\n",
      "Epoch 508. loss = 0.080378\n",
      "Epoch 509. loss = 0.080130\n",
      "Epoch 510. loss = 0.079883\n",
      "Epoch 511. loss = 0.079637\n",
      "Epoch 512. loss = 0.079393\n",
      "Epoch 513. loss = 0.079150\n",
      "Epoch 514. loss = 0.078909\n",
      "Epoch 515. loss = 0.078668\n",
      "Epoch 516. loss = 0.078430\n",
      "Epoch 517. loss = 0.078192\n",
      "Epoch 518. loss = 0.077956\n",
      "Epoch 519. loss = 0.077722\n",
      "Epoch 520. loss = 0.077488\n",
      "Epoch 521. loss = 0.077256\n",
      "Epoch 522. loss = 0.077025\n",
      "Epoch 523. loss = 0.076796\n",
      "Epoch 524. loss = 0.076568\n",
      "Epoch 525. loss = 0.076341\n",
      "Epoch 526. loss = 0.076115\n",
      "Epoch 527. loss = 0.075890\n",
      "Epoch 528. loss = 0.075667\n",
      "Epoch 529. loss = 0.075445\n",
      "Epoch 530. loss = 0.075224\n",
      "Epoch 531. loss = 0.075004\n",
      "Epoch 532. loss = 0.074786\n",
      "Epoch 533. loss = 0.074569\n",
      "Epoch 534. loss = 0.074353\n",
      "Epoch 535. loss = 0.074138\n",
      "Epoch 536. loss = 0.073924\n",
      "Epoch 537. loss = 0.073712\n",
      "Epoch 538. loss = 0.073500\n",
      "Epoch 539. loss = 0.073290\n",
      "Epoch 540. loss = 0.073081\n",
      "Epoch 541. loss = 0.072872\n",
      "Epoch 542. loss = 0.072665\n",
      "Epoch 543. loss = 0.072459\n",
      "Epoch 544. loss = 0.072255\n",
      "Epoch 545. loss = 0.072051\n",
      "Epoch 546. loss = 0.071848\n",
      "Epoch 547. loss = 0.071647\n",
      "Epoch 548. loss = 0.071446\n",
      "Epoch 549. loss = 0.071247\n",
      "Epoch 550. loss = 0.071048\n",
      "Epoch 551. loss = 0.070851\n",
      "Epoch 552. loss = 0.070654\n",
      "Epoch 553. loss = 0.070459\n",
      "Epoch 554. loss = 0.070264\n",
      "Epoch 555. loss = 0.070071\n",
      "Epoch 556. loss = 0.069878\n",
      "Epoch 557. loss = 0.069687\n",
      "Epoch 558. loss = 0.069497\n",
      "Epoch 559. loss = 0.069307\n",
      "Epoch 560. loss = 0.069119\n",
      "Epoch 561. loss = 0.068931\n",
      "Epoch 562. loss = 0.068744\n",
      "Epoch 563. loss = 0.068559\n",
      "Epoch 564. loss = 0.068374\n",
      "Epoch 565. loss = 0.068190\n",
      "Epoch 566. loss = 0.068007\n",
      "Epoch 567. loss = 0.067825\n",
      "Epoch 568. loss = 0.067644\n",
      "Epoch 569. loss = 0.067464\n",
      "Epoch 570. loss = 0.067284\n",
      "Epoch 571. loss = 0.067106\n",
      "Epoch 572. loss = 0.066929\n",
      "Epoch 573. loss = 0.066752\n",
      "Epoch 574. loss = 0.066576\n",
      "Epoch 575. loss = 0.066401\n",
      "Epoch 576. loss = 0.066227\n",
      "Epoch 577. loss = 0.066054\n",
      "Epoch 578. loss = 0.065881\n",
      "Epoch 579. loss = 0.065710\n",
      "Epoch 580. loss = 0.065539\n",
      "Epoch 581. loss = 0.065369\n",
      "Epoch 582. loss = 0.065200\n",
      "Epoch 583. loss = 0.065032\n",
      "Epoch 584. loss = 0.064864\n",
      "Epoch 585. loss = 0.064698\n",
      "Epoch 586. loss = 0.064532\n",
      "Epoch 587. loss = 0.064367\n",
      "Epoch 588. loss = 0.064202\n",
      "Epoch 589. loss = 0.064039\n",
      "Epoch 590. loss = 0.063876\n",
      "Epoch 591. loss = 0.063714\n",
      "Epoch 592. loss = 0.063553\n",
      "Epoch 593. loss = 0.063392\n",
      "Epoch 594. loss = 0.063233\n",
      "Epoch 595. loss = 0.063074\n",
      "Epoch 596. loss = 0.062915\n",
      "Epoch 597. loss = 0.062758\n",
      "Epoch 598. loss = 0.062601\n",
      "Epoch 599. loss = 0.062445\n",
      "Epoch 600. loss = 0.062290\n",
      "Epoch 601. loss = 0.062135\n",
      "Epoch 602. loss = 0.061981\n",
      "Epoch 603. loss = 0.061828\n",
      "Epoch 604. loss = 0.061676\n",
      "Epoch 605. loss = 0.061524\n",
      "Epoch 606. loss = 0.061373\n",
      "Epoch 607. loss = 0.061223\n",
      "Epoch 608. loss = 0.061073\n",
      "Epoch 609. loss = 0.060924\n",
      "Epoch 610. loss = 0.060775\n",
      "Epoch 611. loss = 0.060628\n",
      "Epoch 612. loss = 0.060481\n",
      "Epoch 613. loss = 0.060334\n",
      "Epoch 614. loss = 0.060189\n",
      "Epoch 615. loss = 0.060043\n",
      "Epoch 616. loss = 0.059899\n",
      "Epoch 617. loss = 0.059755\n",
      "Epoch 618. loss = 0.059612\n",
      "Epoch 619. loss = 0.059470\n",
      "Epoch 620. loss = 0.059328\n",
      "Epoch 621. loss = 0.059187\n",
      "Epoch 622. loss = 0.059046\n",
      "Epoch 623. loss = 0.058906\n",
      "Epoch 624. loss = 0.058767\n",
      "Epoch 625. loss = 0.058628\n",
      "Epoch 626. loss = 0.058490\n",
      "Epoch 627. loss = 0.058352\n",
      "Epoch 628. loss = 0.058215\n",
      "Epoch 629. loss = 0.058079\n",
      "Epoch 630. loss = 0.057943\n",
      "Epoch 631. loss = 0.057808\n",
      "Epoch 632. loss = 0.057674\n",
      "Epoch 633. loss = 0.057540\n",
      "Epoch 634. loss = 0.057406\n",
      "Epoch 635. loss = 0.057273\n",
      "Epoch 636. loss = 0.057141\n",
      "Epoch 637. loss = 0.057009\n",
      "Epoch 638. loss = 0.056878\n",
      "Epoch 639. loss = 0.056748\n",
      "Epoch 640. loss = 0.056617\n",
      "Epoch 641. loss = 0.056488\n",
      "Epoch 642. loss = 0.056359\n",
      "Epoch 643. loss = 0.056230\n",
      "Epoch 644. loss = 0.056103\n",
      "Epoch 645. loss = 0.055976\n",
      "Epoch 646. loss = 0.055849\n",
      "Epoch 647. loss = 0.055722\n",
      "Epoch 648. loss = 0.055597\n",
      "Epoch 649. loss = 0.055471\n",
      "Epoch 650. loss = 0.055347\n",
      "Epoch 651. loss = 0.055223\n",
      "Epoch 652. loss = 0.055099\n",
      "Epoch 653. loss = 0.054976\n",
      "Epoch 654. loss = 0.054853\n",
      "Epoch 655. loss = 0.054731\n",
      "Epoch 656. loss = 0.054609\n",
      "Epoch 657. loss = 0.054488\n",
      "Epoch 658. loss = 0.054368\n",
      "Epoch 659. loss = 0.054248\n",
      "Epoch 660. loss = 0.054128\n",
      "Epoch 661. loss = 0.054009\n",
      "Epoch 662. loss = 0.053890\n",
      "Epoch 663. loss = 0.053772\n",
      "Epoch 664. loss = 0.053654\n",
      "Epoch 665. loss = 0.053537\n",
      "Epoch 666. loss = 0.053420\n",
      "Epoch 667. loss = 0.053304\n",
      "Epoch 668. loss = 0.053188\n",
      "Epoch 669. loss = 0.053073\n",
      "Epoch 670. loss = 0.052958\n",
      "Epoch 671. loss = 0.052844\n",
      "Epoch 672. loss = 0.052730\n",
      "Epoch 673. loss = 0.052616\n",
      "Epoch 674. loss = 0.052503\n",
      "Epoch 675. loss = 0.052391\n",
      "Epoch 676. loss = 0.052278\n",
      "Epoch 677. loss = 0.052167\n",
      "Epoch 678. loss = 0.052055\n",
      "Epoch 679. loss = 0.051945\n",
      "Epoch 680. loss = 0.051834\n",
      "Epoch 681. loss = 0.051724\n",
      "Epoch 682. loss = 0.051615\n",
      "Epoch 683. loss = 0.051506\n",
      "Epoch 684. loss = 0.051397\n",
      "Epoch 685. loss = 0.051289\n",
      "Epoch 686. loss = 0.051181\n",
      "Epoch 687. loss = 0.051073\n",
      "Epoch 688. loss = 0.050966\n",
      "Epoch 689. loss = 0.050860\n",
      "Epoch 690. loss = 0.050754\n",
      "Epoch 691. loss = 0.050648\n",
      "Epoch 692. loss = 0.050543\n",
      "Epoch 693. loss = 0.050437\n",
      "Epoch 694. loss = 0.050333\n",
      "Epoch 695. loss = 0.050229\n",
      "Epoch 696. loss = 0.050125\n",
      "Epoch 697. loss = 0.050022\n",
      "Epoch 698. loss = 0.049919\n",
      "Epoch 699. loss = 0.049816\n",
      "Epoch 700. loss = 0.049714\n",
      "Epoch 701. loss = 0.049612\n",
      "Epoch 702. loss = 0.049511\n",
      "Epoch 703. loss = 0.049410\n",
      "Epoch 704. loss = 0.049309\n",
      "Epoch 705. loss = 0.049209\n",
      "Epoch 706. loss = 0.049109\n",
      "Epoch 707. loss = 0.049010\n",
      "Epoch 708. loss = 0.048910\n",
      "Epoch 709. loss = 0.048812\n",
      "Epoch 710. loss = 0.048713\n",
      "Epoch 711. loss = 0.048615\n",
      "Epoch 712. loss = 0.048517\n",
      "Epoch 713. loss = 0.048420\n",
      "Epoch 714. loss = 0.048323\n",
      "Epoch 715. loss = 0.048227\n",
      "Epoch 716. loss = 0.048130\n",
      "Epoch 717. loss = 0.048035\n",
      "Epoch 718. loss = 0.047939\n",
      "Epoch 719. loss = 0.047844\n",
      "Epoch 720. loss = 0.047749\n",
      "Epoch 721. loss = 0.047654\n",
      "Epoch 722. loss = 0.047560\n",
      "Epoch 723. loss = 0.047466\n",
      "Epoch 724. loss = 0.047373\n",
      "Epoch 725. loss = 0.047280\n",
      "Epoch 726. loss = 0.047187\n",
      "Epoch 727. loss = 0.047095\n",
      "Epoch 728. loss = 0.047003\n",
      "Epoch 729. loss = 0.046911\n",
      "Epoch 730. loss = 0.046819\n",
      "Epoch 731. loss = 0.046728\n",
      "Epoch 732. loss = 0.046637\n",
      "Epoch 733. loss = 0.046547\n",
      "Epoch 734. loss = 0.046457\n",
      "Epoch 735. loss = 0.046367\n",
      "Epoch 736. loss = 0.046277\n",
      "Epoch 737. loss = 0.046188\n",
      "Epoch 738. loss = 0.046099\n",
      "Epoch 739. loss = 0.046011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 740. loss = 0.045923\n",
      "Epoch 741. loss = 0.045835\n",
      "Epoch 742. loss = 0.045747\n",
      "Epoch 743. loss = 0.045660\n",
      "Epoch 744. loss = 0.045573\n",
      "Epoch 745. loss = 0.045486\n",
      "Epoch 746. loss = 0.045400\n",
      "Epoch 747. loss = 0.045314\n",
      "Epoch 748. loss = 0.045228\n",
      "Epoch 749. loss = 0.045142\n",
      "Epoch 750. loss = 0.045057\n",
      "Epoch 751. loss = 0.044972\n",
      "Epoch 752. loss = 0.044888\n",
      "Epoch 753. loss = 0.044803\n",
      "Epoch 754. loss = 0.044719\n",
      "Epoch 755. loss = 0.044635\n",
      "Epoch 756. loss = 0.044552\n",
      "Epoch 757. loss = 0.044469\n",
      "Epoch 758. loss = 0.044386\n",
      "Epoch 759. loss = 0.044303\n",
      "Epoch 760. loss = 0.044221\n",
      "Epoch 761. loss = 0.044139\n",
      "Epoch 762. loss = 0.044057\n",
      "Epoch 763. loss = 0.043976\n",
      "Epoch 764. loss = 0.043895\n",
      "Epoch 765. loss = 0.043814\n",
      "Epoch 766. loss = 0.043733\n",
      "Epoch 767. loss = 0.043653\n",
      "Epoch 768. loss = 0.043573\n",
      "Epoch 769. loss = 0.043493\n",
      "Epoch 770. loss = 0.043413\n",
      "Epoch 771. loss = 0.043334\n",
      "Epoch 772. loss = 0.043255\n",
      "Epoch 773. loss = 0.043176\n",
      "Epoch 774. loss = 0.043098\n",
      "Epoch 775. loss = 0.043020\n",
      "Epoch 776. loss = 0.042942\n",
      "Epoch 777. loss = 0.042864\n",
      "Epoch 778. loss = 0.042787\n",
      "Epoch 779. loss = 0.042709\n",
      "Epoch 780. loss = 0.042632\n",
      "Epoch 781. loss = 0.042556\n",
      "Epoch 782. loss = 0.042479\n",
      "Epoch 783. loss = 0.042403\n",
      "Epoch 784. loss = 0.042327\n",
      "Epoch 785. loss = 0.042251\n",
      "Epoch 786. loss = 0.042176\n",
      "Epoch 787. loss = 0.042101\n",
      "Epoch 788. loss = 0.042026\n",
      "Epoch 789. loss = 0.041951\n",
      "Epoch 790. loss = 0.041877\n",
      "Epoch 791. loss = 0.041803\n",
      "Epoch 792. loss = 0.041729\n",
      "Epoch 793. loss = 0.041655\n",
      "Epoch 794. loss = 0.041581\n",
      "Epoch 795. loss = 0.041508\n",
      "Epoch 796. loss = 0.041435\n",
      "Epoch 797. loss = 0.041362\n",
      "Epoch 798. loss = 0.041290\n",
      "Epoch 799. loss = 0.041218\n",
      "Epoch 800. loss = 0.041146\n",
      "Epoch 801. loss = 0.041074\n",
      "Epoch 802. loss = 0.041002\n",
      "Epoch 803. loss = 0.040931\n",
      "Epoch 804. loss = 0.040860\n",
      "Epoch 805. loss = 0.040789\n",
      "Epoch 806. loss = 0.040718\n",
      "Epoch 807. loss = 0.040648\n",
      "Epoch 808. loss = 0.040577\n",
      "Epoch 809. loss = 0.040507\n",
      "Epoch 810. loss = 0.040437\n",
      "Epoch 811. loss = 0.040368\n",
      "Epoch 812. loss = 0.040298\n",
      "Epoch 813. loss = 0.040229\n",
      "Epoch 814. loss = 0.040160\n",
      "Epoch 815. loss = 0.040092\n",
      "Epoch 816. loss = 0.040023\n",
      "Epoch 817. loss = 0.039955\n",
      "Epoch 818. loss = 0.039887\n",
      "Epoch 819. loss = 0.039819\n",
      "Epoch 820. loss = 0.039751\n",
      "Epoch 821. loss = 0.039684\n",
      "Epoch 822. loss = 0.039617\n",
      "Epoch 823. loss = 0.039550\n",
      "Epoch 824. loss = 0.039483\n",
      "Epoch 825. loss = 0.039416\n",
      "Epoch 826. loss = 0.039350\n",
      "Epoch 827. loss = 0.039284\n",
      "Epoch 828. loss = 0.039218\n",
      "Epoch 829. loss = 0.039152\n",
      "Epoch 830. loss = 0.039086\n",
      "Epoch 831. loss = 0.039021\n",
      "Epoch 832. loss = 0.038956\n",
      "Epoch 833. loss = 0.038891\n",
      "Epoch 834. loss = 0.038826\n",
      "Epoch 835. loss = 0.038762\n",
      "Epoch 836. loss = 0.038697\n",
      "Epoch 837. loss = 0.038633\n",
      "Epoch 838. loss = 0.038569\n",
      "Epoch 839. loss = 0.038505\n",
      "Epoch 840. loss = 0.038442\n",
      "Epoch 841. loss = 0.038378\n",
      "Epoch 842. loss = 0.038315\n",
      "Epoch 843. loss = 0.038252\n",
      "Epoch 844. loss = 0.038189\n",
      "Epoch 845. loss = 0.038127\n",
      "Epoch 846. loss = 0.038064\n",
      "Epoch 847. loss = 0.038002\n",
      "Epoch 848. loss = 0.037940\n",
      "Epoch 849. loss = 0.037878\n",
      "Epoch 850. loss = 0.037816\n",
      "Epoch 851. loss = 0.037755\n",
      "Epoch 852. loss = 0.037693\n",
      "Epoch 853. loss = 0.037632\n",
      "Epoch 854. loss = 0.037571\n",
      "Epoch 855. loss = 0.037510\n",
      "Epoch 856. loss = 0.037450\n",
      "Epoch 857. loss = 0.037389\n",
      "Epoch 858. loss = 0.037329\n",
      "Epoch 859. loss = 0.037269\n",
      "Epoch 860. loss = 0.037209\n",
      "Epoch 861. loss = 0.037149\n",
      "Epoch 862. loss = 0.037090\n",
      "Epoch 863. loss = 0.037030\n",
      "Epoch 864. loss = 0.036971\n",
      "Epoch 865. loss = 0.036912\n",
      "Epoch 866. loss = 0.036853\n",
      "Epoch 867. loss = 0.036795\n",
      "Epoch 868. loss = 0.036736\n",
      "Epoch 869. loss = 0.036678\n",
      "Epoch 870. loss = 0.036620\n",
      "Epoch 871. loss = 0.036561\n",
      "Epoch 872. loss = 0.036504\n",
      "Epoch 873. loss = 0.036446\n",
      "Epoch 874. loss = 0.036388\n",
      "Epoch 875. loss = 0.036331\n",
      "Epoch 876. loss = 0.036274\n",
      "Epoch 877. loss = 0.036217\n",
      "Epoch 878. loss = 0.036160\n",
      "Epoch 879. loss = 0.036103\n",
      "Epoch 880. loss = 0.036047\n",
      "Epoch 881. loss = 0.035990\n",
      "Epoch 882. loss = 0.035934\n",
      "Epoch 883. loss = 0.035878\n",
      "Epoch 884. loss = 0.035822\n",
      "Epoch 885. loss = 0.035767\n",
      "Epoch 886. loss = 0.035711\n",
      "Epoch 887. loss = 0.035656\n",
      "Epoch 888. loss = 0.035600\n",
      "Epoch 889. loss = 0.035545\n",
      "Epoch 890. loss = 0.035490\n",
      "Epoch 891. loss = 0.035436\n",
      "Epoch 892. loss = 0.035381\n",
      "Epoch 893. loss = 0.035327\n",
      "Epoch 894. loss = 0.035272\n",
      "Epoch 895. loss = 0.035218\n",
      "Epoch 896. loss = 0.035164\n",
      "Epoch 897. loss = 0.035110\n",
      "Epoch 898. loss = 0.035057\n",
      "Epoch 899. loss = 0.035003\n",
      "Epoch 900. loss = 0.034950\n",
      "Epoch 901. loss = 0.034896\n",
      "Epoch 902. loss = 0.034843\n",
      "Epoch 903. loss = 0.034790\n",
      "Epoch 904. loss = 0.034738\n",
      "Epoch 905. loss = 0.034685\n",
      "Epoch 906. loss = 0.034632\n",
      "Epoch 907. loss = 0.034580\n",
      "Epoch 908. loss = 0.034528\n",
      "Epoch 909. loss = 0.034475\n",
      "Epoch 910. loss = 0.034424\n",
      "Epoch 911. loss = 0.034372\n",
      "Epoch 912. loss = 0.034320\n",
      "Epoch 913. loss = 0.034269\n",
      "Epoch 914. loss = 0.034217\n",
      "Epoch 915. loss = 0.034166\n",
      "Epoch 916. loss = 0.034115\n",
      "Epoch 917. loss = 0.034064\n",
      "Epoch 918. loss = 0.034013\n",
      "Epoch 919. loss = 0.033962\n",
      "Epoch 920. loss = 0.033912\n",
      "Epoch 921. loss = 0.033861\n",
      "Epoch 922. loss = 0.033811\n",
      "Epoch 923. loss = 0.033761\n",
      "Epoch 924. loss = 0.033711\n",
      "Epoch 925. loss = 0.033661\n",
      "Epoch 926. loss = 0.033612\n",
      "Epoch 927. loss = 0.033562\n",
      "Epoch 928. loss = 0.033513\n",
      "Epoch 929. loss = 0.033463\n",
      "Epoch 930. loss = 0.033414\n",
      "Epoch 931. loss = 0.033365\n",
      "Epoch 932. loss = 0.033316\n",
      "Epoch 933. loss = 0.033267\n",
      "Epoch 934. loss = 0.033218\n",
      "Epoch 935. loss = 0.033170\n",
      "Epoch 936. loss = 0.033121\n",
      "Epoch 937. loss = 0.033073\n",
      "Epoch 938. loss = 0.033025\n",
      "Epoch 939. loss = 0.032977\n",
      "Epoch 940. loss = 0.032929\n",
      "Epoch 941. loss = 0.032881\n",
      "Epoch 942. loss = 0.032834\n",
      "Epoch 943. loss = 0.032786\n",
      "Epoch 944. loss = 0.032739\n",
      "Epoch 945. loss = 0.032692\n",
      "Epoch 946. loss = 0.032644\n",
      "Epoch 947. loss = 0.032597\n",
      "Epoch 948. loss = 0.032551\n",
      "Epoch 949. loss = 0.032504\n",
      "Epoch 950. loss = 0.032457\n",
      "Epoch 951. loss = 0.032411\n",
      "Epoch 952. loss = 0.032364\n",
      "Epoch 953. loss = 0.032318\n",
      "Epoch 954. loss = 0.032272\n",
      "Epoch 955. loss = 0.032226\n",
      "Epoch 956. loss = 0.032180\n",
      "Epoch 957. loss = 0.032134\n",
      "Epoch 958. loss = 0.032088\n",
      "Epoch 959. loss = 0.032043\n",
      "Epoch 960. loss = 0.031997\n",
      "Epoch 961. loss = 0.031952\n",
      "Epoch 962. loss = 0.031907\n",
      "Epoch 963. loss = 0.031861\n",
      "Epoch 964. loss = 0.031817\n",
      "Epoch 965. loss = 0.031772\n",
      "Epoch 966. loss = 0.031727\n",
      "Epoch 967. loss = 0.031682\n",
      "Epoch 968. loss = 0.031638\n",
      "Epoch 969. loss = 0.031593\n",
      "Epoch 970. loss = 0.031549\n",
      "Epoch 971. loss = 0.031505\n",
      "Epoch 972. loss = 0.031461\n",
      "Epoch 973. loss = 0.031417\n",
      "Epoch 974. loss = 0.031373\n",
      "Epoch 975. loss = 0.031329\n",
      "Epoch 976. loss = 0.031286\n",
      "Epoch 977. loss = 0.031242\n",
      "Epoch 978. loss = 0.031199\n",
      "Epoch 979. loss = 0.031156\n",
      "Epoch 980. loss = 0.031112\n",
      "Epoch 981. loss = 0.031069\n",
      "Epoch 982. loss = 0.031026\n",
      "Epoch 983. loss = 0.030983\n",
      "Epoch 984. loss = 0.030941\n",
      "Epoch 985. loss = 0.030898\n",
      "Epoch 986. loss = 0.030855\n",
      "Epoch 987. loss = 0.030813\n",
      "Epoch 988. loss = 0.030771\n",
      "Epoch 989. loss = 0.030728\n",
      "Epoch 990. loss = 0.030686\n",
      "Epoch 991. loss = 0.030644\n",
      "Epoch 992. loss = 0.030602\n",
      "Epoch 993. loss = 0.030561\n",
      "Epoch 994. loss = 0.030519\n",
      "Epoch 995. loss = 0.030477\n",
      "Epoch 996. loss = 0.030436\n",
      "Epoch 997. loss = 0.030394\n",
      "Epoch 998. loss = 0.030353\n",
      "Epoch 999. loss = 0.030312\n",
      "0.9890331029891968\n"
     ]
    }
   ],
   "source": [
    "# XOR problem\n",
    "\n",
    "import dynet as dy\n",
    "import random\n",
    "\n",
    "data = [([0,1],0),\n",
    "        ([1,0],0),\n",
    "        ([0,0],1),\n",
    "        ([1,1],1)]\n",
    "\n",
    "model = dy.Model()\n",
    "pU = model.add_parameters((4,2))\n",
    "pb = model.add_parameters(4)\n",
    "pv = model.add_parameters(4)\n",
    "\n",
    "trainer = dy.SimpleSGDTrainer(model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EPOCHS = 1000\n",
    "\n",
    "for epoch in range (EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    for x,y in data:\n",
    "        dy.renew_cg()\n",
    "        U = dy.parameter(pU)\n",
    "        b = dy.parameter(pb)\n",
    "        v = dy.parameter(pv)\n",
    "        x = dy.inputVector(x)\n",
    "        \n",
    "        # prediction\n",
    "        \n",
    "        yhat = dy.logistic(dy.dot_product(v,dy.tanh(U * x + b)))\n",
    "        \n",
    "        # calculate loss\n",
    "        if y == 0:\n",
    "            loss = -dy.log(1 - yhat)\n",
    "        else:\n",
    "            loss = -dy.log(yhat)\n",
    "            \n",
    "        epoch_loss += loss.scalar_value()\n",
    "        \n",
    "        loss.backward()\n",
    "        trainer.update()\n",
    "        \n",
    "    print(\"Epoch %d. loss = %f\" % (epoch, epoch_loss))\n",
    "    \n",
    "inputData = [0,0]\n",
    "i = dy.inputVector(inputData)\n",
    "yhat = dy.logistic(dy.dot_product(v,dy.tanh(U * x + b)))\n",
    "\n",
    "print(yhat.value())\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File tagged_plots_movielens.csv does not exist: 'tagged_plots_movielens.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4c0c2104d168>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi2l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tagged_plots_movielens.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mw2i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_glove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-4c0c2104d168>\u001b[0m in \u001b[0;36mread_dataset\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reading dataset...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     tags_index = {\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File tagged_plots_movielens.csv does not exist: 'tagged_plots_movielens.csv'"
     ]
    }
   ],
   "source": [
    "# Deep Averaging Network (Look-up table)\n",
    "\n",
    "import dynet as dy\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "\n",
    "l2i = {}\n",
    "i2l = {}\n",
    "\n",
    "V = 1000\n",
    "HID = 20\n",
    "EDIM = 50\n",
    "NOUT = 5\n",
    "\n",
    "\n",
    "def predict_labels(doc):\n",
    "    x = encode_doc(doc)\n",
    "    h= layer1(x)\n",
    "    y = layer2(h)\n",
    "    \n",
    "    return dy.softmax(y)\n",
    "\n",
    "\n",
    "def layer1(x):\n",
    "    W = dy.parameter(pW1)\n",
    "    b = dy.parameter(pb1)\n",
    "    \n",
    "    return dy.tanh(W * x + b)\n",
    "\n",
    "def layer2(x):\n",
    "    W = dy.parameter(pW2)\n",
    "    b = dy.parameter(pb2)\n",
    "    \n",
    "    return dy.tanh(W * x + b)\n",
    "\n",
    "def encode_doc(doc):\n",
    "    doc = [w2i[w] for w in doc]\n",
    "    embs = [E[idx] for idx in doc]\n",
    "    \n",
    "    return dy.esum(embs)\n",
    "\n",
    "def do_loss(probs, label):\n",
    "    label = l2i[label]\n",
    "    \n",
    "    return -dy.log(dy.pick(probs,label))\n",
    "\n",
    "def classify(doc):\n",
    "    dy.renew_cg()\n",
    "    probs = predict_labels(doc)\n",
    "    \n",
    "    vals = probs.npvalue()\n",
    "    \n",
    "    return i2l[np.argmax(vals)]\n",
    "\n",
    "def read_dataset(fname):\n",
    "    print('Reading dataset...')\n",
    "    df = pd.read_csv(fname)\n",
    "\n",
    "    tags_index = {\n",
    "        \"sci-fi\": 1,\n",
    "        \"action\": 2,\n",
    "        \"comedy\": 3,\n",
    "        \"fantasy\": 4,\n",
    "        \"animation\": 5,\n",
    "        \"romance\": 6,\n",
    "    }\n",
    "    \n",
    "    index_tags = {\n",
    "        1 : \"sci-fi\",\n",
    "        2 : \"action\",\n",
    "        3 : \"comedy\",\n",
    "        4 : \"fantasy\",\n",
    "        5 : \"animation\",\n",
    "        6 : \"romance\",\n",
    "    }\n",
    "   \n",
    "    data = []\n",
    "    with open(fname) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if line_count == 0:\n",
    "                line_count += 1\n",
    "            else:\n",
    "                data.append([row[2],tags_index[row[3].strip(\"\\n\")]])\n",
    "                line_count += 1\n",
    "\n",
    "    return data, tags_index,index_tags\n",
    "\n",
    "def read_glove():\n",
    "    print('Loading word vectors...')\n",
    "    word2vec = {}\n",
    "    embedding = []\n",
    "    embeds = []\n",
    "    word2idx = {}\n",
    "    with open('glove.6B.50d.txt', encoding='utf-8') as f:\n",
    "      for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        word2idx[word] = len(embeds)\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "        embeds.append(vec)\n",
    "    return np.array(embeds),word2idx\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "data, l2i, i2l = read_dataset(\"tagged_plots_movielens.csv\")\n",
    "embedding , w2i = read_glove()\n",
    "\n",
    "pW1 = model.add_parameters((HID, EDIM))\n",
    "pb1 = model.add_parameters(HID)\n",
    "pW2 = model.add_parameters((NOUT, HID))\n",
    "pb2 = model.add_parameters(NOUT)\n",
    "E = model.add_lookup_parameters((len(w2idx), EDIM),init =  embedding )\n",
    "\n",
    "for (doc, label) in data:\n",
    "    dy.renew_cg()\n",
    "    probs = predict_labels(doc)\n",
    "    \n",
    "    loss = do_loss(probs,label)\n",
    "    loss.forward()\n",
    "    loss.backward()\n",
    "    trainer.update()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
