{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dynet as dy\n",
    "import json\n",
    "from glove import Glove\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorDim = 50 # 50 100 200 300\n",
    "ngram = 3 # 3->trigram   2->bigram  1->unigram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset...\n",
      "Just Finished Reading dataset...\n",
      "Poem Count: 93265\n",
      "[['bos', 'abandoned', 'drive-in'], ['abandoned', 'drive-in', 'eol'], ['drive-in', 'eol', 'lit'], ['eol', 'lit', 'by'], ['lit', 'by', 'the'], ['by', 'the', 'glow'], ['the', 'glow', 'of'], ['glow', 'of', 'pink'], ['of', 'pink', 'light'], ['pink', 'light', 'eol'], ['light', 'eol', 'from'], ['eol', 'from', 'a'], ['from', 'a', 'waning'], ['a', 'waning', 'day'], ['waning', 'day', 'eos']]\n"
     ]
    }
   ],
   "source": [
    "# bos # begin of sentence\n",
    "# eos # end of sentence\n",
    "# eol # end of line\n",
    "# bol # begin of line\n",
    "\n",
    "def read_dataset(fpath):\n",
    "    print(\"Reading dataset...\")\n",
    "    poems=[]\n",
    "    with open(fpath) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        for d in data:\n",
    "            poem = d[\"poem\"]\n",
    "            #poem = poem.replace(\"\\n\",\" eol bol \")\n",
    "            #poem = poem.lower() -- dataset already lowercased\n",
    "            poem = poem.replace(\"\\n\",\" eol \")\n",
    "            poem = poem.replace(\".\",\" \")\n",
    "            poem = poem.replace(\":\",\" \")\n",
    "            poem = poem.replace(\"?\",\" \")\n",
    "            #poem = \"bos bol \"+poem+\" eol eos\"\n",
    "            poem = \"bos \"+poem+\" eos\"\n",
    "            poem = poem.split()\n",
    "            poems.append([])\n",
    "            for i in range(len(poem)-ngram+1):\n",
    "                poems[-1].append(poem[i:i+ngram])\n",
    "    print(\"Just Finished Reading dataset...\")\n",
    "    return poems\n",
    "\n",
    "poems2 = read_dataset('unim_poem.json')\n",
    "print(\"Poem Count:\",len(poems2))\n",
    "print(poems2[-1])  \n",
    "\n",
    "# replace(\".:?\",\" \")\n",
    "# , ; ! \" | yok\n",
    "# ' - 're 's n't 've 'll ellemedim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Finished Loading word vectors...\n",
      "(400001, 50)\n",
      "20000050\n"
     ]
    }
   ],
   "source": [
    "def read_glove(fpath):\n",
    "    print('Loading word vectors...')\n",
    "    word2vec = {}\n",
    "    embeds = []\n",
    "    word2idx = {}\n",
    "    with open(fpath, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            word2idx[word] = len(embeds)\n",
    "            vec = np.asarray(values[1:], dtype='float32')\n",
    "            word2vec[word] = vec\n",
    "            embeds.append(vec)\n",
    "            \n",
    "    mean = np.array(embeds).mean(axis=0,dtype='float32')\n",
    "    word2vec[\"mmeann\"]=mean\n",
    "    embeds.append(mean)\n",
    "    \n",
    "    print(\"Finished Loading word vectors...\")\n",
    "    return np.array(embeds),word2idx,word2vec\n",
    "\n",
    "embedding,w2i, w2v= read_glove('glovo/glove.6B.'+str(vectorDim)+'d.txt')\n",
    "#w2v[\"mmeann\"]=embedding.mean(axis=0) # if the word doesn't occur in vocab, it will take mean value\n",
    "print(embedding.shape)\n",
    "print(embedding.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(w2v['the'])\n",
    "# print(w2v['mmeann'])\n",
    "# print(w2v['the']+w2v['mmeann'])\n",
    "# print(np.zeros(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. loss = 126.885050\n",
      "Epoch 2. loss = 126.756240\n",
      "Epoch 3. loss = 126.750117\n",
      "Epoch 4. loss = 126.726632\n",
      "Epoch 5. loss = 126.747889\n",
      "Epoch 6. loss = 126.990369\n",
      "Epoch 7. loss = 126.819649\n",
      "Epoch 8. loss = 126.515956\n",
      "Epoch 9. loss = 126.054736\n",
      "Epoch 10. loss = 126.437603\n"
     ]
    }
   ],
   "source": [
    "def sumvec(w2v,words):\n",
    "    _temp = np.zeros(50)\n",
    "    for w in words:\n",
    "        try:\n",
    "            _temp+=w2v[w]\n",
    "        except:\n",
    "            _temp+=w2v[\"mmeann\"]\n",
    "    return _temp\n",
    "\n",
    "h = 100 # HiddenUnit\n",
    "m = vectorDim\n",
    "OutUnit = 100\n",
    "\n",
    "EPOCH = 10\n",
    "\n",
    "_model = dy.Model()\n",
    "_pW1 = _model.add_parameters((h, m))\n",
    "_pb1 = _model.add_parameters(h)\n",
    "_pW2 = _model.add_parameters((m, h))\n",
    "_pb2 = _model.add_parameters(m)\n",
    "_trainer = dy.SimpleSGDTrainer(_model)\n",
    "\n",
    "\n",
    "poems = poems2[:250]\n",
    "for epoch in range(1, EPOCH + 1):\n",
    "    epoch_loss = 0.0\n",
    "    for p in poems:\n",
    "        for gram in p:\n",
    "            x=sumvec(w2v,gram[:ngram-1])\n",
    "            y=max(sumvec(w2v,gram[-1:]))\n",
    "            dy.renew_cg()\n",
    "            x = dy.inputVector(x)\n",
    "            input_layer = dy.tanh(_pW1 * x + _pb1)\n",
    "            hidden_layer = _pW2 * input_layer + _pb2\n",
    "            output_layer = dy.softmax(hidden_layer)\n",
    "            loss = dy.pickneglogsoftmax(output_layer, y)\n",
    "            epoch_loss += loss.scalar_value()\n",
    "            loss.backward()\n",
    "            _trainer.update()\n",
    "    print(\"Epoch %d. loss = %f\" % (epoch, epoch_loss/len(poems)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(doc,b,U,d,H):\n",
    "    x = encode_doc(doc)\n",
    "    _h= layer1(x,H,d)\n",
    "    y = b+U*_h\n",
    "    \n",
    "    return dy.softmax(y)\n",
    "\n",
    "\n",
    "def layer1(x,H,d):\n",
    "    _H = dy.parameter(H)\n",
    "    _d = dy.parameter(d)\n",
    "    \n",
    "    return dy.tanh(_H * x + _d)\n",
    "\n",
    "def encode_doc(doc):\n",
    "    doc2=[]\n",
    "    embs=[]\n",
    "    for w in doc:\n",
    "        try:\n",
    "            doc2.append(w2i[w])\n",
    "        except:\n",
    "            doc2.append(w2i[\"mmeann\"])\n",
    "    for idx in doc2:\n",
    "        embs.append(E[idx])\n",
    "    \n",
    "    return dy.esum(embs)\n",
    "\n",
    "\n",
    "def do_loss(probs, next_word, w2v):\n",
    "    try:\n",
    "        next_word_vector = w2v[next_word]\n",
    "    except:\n",
    "        next_word_vector = w2v[\"mmeann\"]\n",
    "\n",
    "    return -dy.log(dy.pick_batch_elem(probs,next_word_vector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-50e68039bd56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-91e61f11da55>\u001b[0m in \u001b[0;36mdo_loss\u001b[0;34m(probs, next_word, w2v)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mnext_word_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2v\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mmeann\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpick_batch_elem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnext_word_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m_dynet.pyx\u001b[0m in \u001b[0;36m_dynet.pick_batch_elem\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "model = dy.Model()\n",
    "\n",
    "trainer = dy.SimpleSGDTrainer(model)\n",
    "\n",
    "h = 100 # HiddenUnit\n",
    "m = vectorDim\n",
    "OutUnit = 100\n",
    "\n",
    "H = model.add_parameters((h, m))\n",
    "d = model.add_parameters(h)\n",
    "U = model.add_parameters((m, h))\n",
    "b = model.add_parameters(m)\n",
    "\n",
    "E = model.add_lookup_parameters((len(w2i), HiddenDim),init =  embedding )\n",
    "\n",
    "for p in poems:\n",
    "    for gram in p:\n",
    "        dy.renew_cg()\n",
    "        probs = predict_labels(gram[:ngram-1],b,U,d,H)\n",
    "        \n",
    "        loss = do_loss(probs,gram[-1],w2v)\n",
    "        loss.forward()\n",
    "        loss.backward()\n",
    "        trainer.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "LookupParameter /_4\n",
      "(400000, 50)\n",
      "expression 9/0\n",
      "expression 10/0\n"
     ]
    }
   ],
   "source": [
    "print(len(w2i))\n",
    "print(E)\n",
    "print(E.shape())\n",
    "print(E[0])\n",
    "print(E[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit418366e376b14bef830f0db0b5287b85"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
