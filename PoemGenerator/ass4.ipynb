{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dynet as dy\n",
    "import json\n",
    "from glove import Glove\n",
    "import numpy as np\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "import random\n",
    "\n",
    "\n",
    "def DefaultdictInside():\n",
    "    return [defaultdict(int),0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorDim = 50 # 50 100 200 300\n",
    "ngram = 3 # 3->trigram   2->bigram  1->unigram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset...\n",
      "Just Finished Reading dataset...\n",
      "Poem Count: 100\n",
      "Unique Gram: 3438\n",
      "Total Word: 5123\n"
     ]
    }
   ],
   "source": [
    "# bos # begin of sentence\n",
    "# eos # end of sentence\n",
    "# eol # end of line\n",
    "# bol # begin of line\n",
    "\n",
    "def read_dataset(fpath):\n",
    "    print(\"Reading dataset...\")\n",
    "    poems=[]\n",
    "    n_grams_dict=defaultdict(DefaultdictInside)\n",
    "    with open(fpath) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        for d in data:\n",
    "            if d[\"id\"]==100:\n",
    "                break\n",
    "            poem = d[\"poem\"]\n",
    "            poem = poem.replace(\"\\n\",\" eol bol \")\n",
    "            #poem = poem.lower() -- dataset already lowercased\n",
    "            #poem = poem.replace(\"\\n\",\" eol \")\n",
    "            poem = poem.replace(\".\",\" \")\n",
    "            poem = poem.replace(\":\",\" \")\n",
    "            poem = poem.replace(\"?\",\" \")\n",
    "            poem = \"bos bol \"+poem+\" eol eos\"\n",
    "            #poem = \"bos \"+poem+\" eos\"\n",
    "            poem = poem.split()\n",
    "            poems.append([])\n",
    "            for i in range(len(poem)-ngram+1):\n",
    "                poems[-1].append(poem[i:i+ngram])\n",
    "                \n",
    "                prev_gram = poems[-1][-1][:ngram-1]\n",
    "                next_gram = poems[-1][-1][-1]    \n",
    "                n_grams_dict[\" \".join(prev_gram)][1]+=1\n",
    "                n_grams_dict[\" \".join(prev_gram)][0][next_gram]+=1\n",
    "            n_grams_dict[\"eos\"][1]+=1\n",
    "            \n",
    "    print(\"Just Finished Reading dataset...\")\n",
    "    return poems,n_grams_dict\n",
    "\n",
    "poems,count_dict= read_dataset('unim_poem.json')\n",
    "print(\"Poem Count:\",len(poems))\n",
    "#print(poems[0])\n",
    "keys = list(count_dict.keys())\n",
    "unique_gram = len(keys)\n",
    "print(\"Unique Gram:\",unique_gram)\n",
    "tot=0\n",
    "for k,v in count_dict.items():\n",
    "    tot += v[1]\n",
    "print(\"Total Word:\",tot)\n",
    "\n",
    "# replace(\".:?\",\" \")\n",
    "# , ; ! \" | yok\n",
    "# ' - 're 's n't 've 'll ellemedim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Finished Loading word vectors...\n",
      "(400001, 50)\n",
      "20000050\n"
     ]
    }
   ],
   "source": [
    "def read_glove(fpath):\n",
    "    print('Loading word vectors...')\n",
    "    word2vec = {}\n",
    "    embeds = []\n",
    "    word2idx = {}\n",
    "    with open(fpath, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            word2idx[word] = len(embeds)\n",
    "            vec = np.asarray(values[1:], dtype='float32')\n",
    "            word2vec[word] = vec\n",
    "            embeds.append(vec)\n",
    "            \n",
    "    mean = np.array(embeds).mean(axis=0,dtype='float32')\n",
    "    word2vec[\"mmeann\"]=mean\n",
    "    embeds.append(mean)\n",
    "    \n",
    "    print(\"Finished Loading word vectors...\")\n",
    "    return np.array(embeds),word2idx,word2vec\n",
    "\n",
    "embedding,w2i, w2v= read_glove('glovo/glove.6B.'+str(vectorDim)+'d.txt')\n",
    "#w2v[\"mmeann\"]=embedding.mean(axis=0) # if the word doesn't occur in vocab, it will take mean value\n",
    "print(embedding.shape)\n",
    "print(embedding.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. loss = 409.005031\n",
      "Epoch 2. loss = 409.005073\n",
      "Epoch 3. loss = 409.005001\n",
      "Epoch 4. loss = 409.005056\n",
      "Epoch 5. loss = 409.005049\n",
      "Epoch 6. loss = 409.004937\n",
      "Epoch 7. loss = 409.004971\n",
      "Epoch 8. loss = 409.004933\n",
      "Epoch 9. loss = 409.004919\n",
      "Epoch 10. loss = 409.004867\n"
     ]
    }
   ],
   "source": [
    "def sumvec(w2v,words):\n",
    "    _temp = np.zeros(50)\n",
    "    for w in words:\n",
    "        try:\n",
    "            _temp+=w2v[w]\n",
    "        except:\n",
    "            _temp+=w2v[\"mmeann\"]\n",
    "    return _temp\n",
    "\n",
    "h = 150 # HiddenUnit\n",
    "m = vectorDim\n",
    "\n",
    "EPOCH = 10\n",
    "\n",
    "_model = dy.Model()\n",
    "_pW1 = _model.add_parameters((h, m))\n",
    "_pb1 = _model.add_parameters(h)\n",
    "_pW2 = _model.add_parameters((unique_gram, h))\n",
    "_pb2 = _model.add_parameters(unique_gram)\n",
    "_trainer = dy.SimpleSGDTrainer(_model)\n",
    "\n",
    "\n",
    "i=0\n",
    "for epoch in range(1, EPOCH + 1):\n",
    "    epoch_loss = 0.0\n",
    "    for p in poems:\n",
    "        for gram in p:\n",
    "            x=sumvec(w2v,gram[:ngram-1])\n",
    "            try:\n",
    "                y=keys.index(gram[-1])\n",
    "            except:\n",
    "                y=random.randint(0,len(keys)-1)\n",
    "            dy.renew_cg()\n",
    "            x = dy.inputVector(x)\n",
    "            input_layer = dy.tanh(_pW1 * x + _pb1)\n",
    "            hidden_layer = _pW2 * input_layer + _pb2\n",
    "            output_layer = dy.softmax(hidden_layer)\n",
    "            loss = dy.pickneglogsoftmax(output_layer, y)\n",
    "            epoch_loss += loss.scalar_value()\n",
    "            loss.backward()\n",
    "            _trainer.update()\n",
    "    print(\"Epoch %d. loss = %f\" % (epoch, epoch_loss/len(poems)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bos bol slabs for the morn old-time step bol families hardens a' left us the morn cares what as able couldn't understand made me you-oh you life eol in her their school hardens a' left us diminished size for these bol haply bol haply balance shake loose-lipped demagogue diminished size the black men judge and trouble of concealing slabs for balance shake wine-dark river cares what cheering crowds cries what cares what diminished size river eol river eol i'll no couldn't understand then someone made me hardens a' made me suit without as able would she made me i'll no as able hardens a' couldn't understand me still hardens a' left us as able o' weel-plac'd bol haply alone eol\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "close=20\n",
    "satir = 4\n",
    "sent = ['bos bol']\n",
    "\n",
    "for _ in range(satir):\n",
    "    while 1:\n",
    "        #while sent[-1]!=\"eol\":# and len(sent)<10:\n",
    "        x=sumvec(w2v,sent[-1])\n",
    "        dy.renew_cg()\n",
    "        x = dy.inputVector(x)\n",
    "        input_layer = dy.tanh(_pW1 * x + _pb1)\n",
    "        hidden_layer = _pW2 * input_layer + _pb2\n",
    "        output_layer = list(dy.softmax(hidden_layer).value())\n",
    "        oen = list(enumerate(output_layer))\n",
    "        oen=sorted(oen,key=itemgetter(1),reverse=True)\n",
    "        rnd = random.randint(0,close-1)\n",
    "        sent.append(keys[oen[rnd][0]])\n",
    "        #print([sent])\n",
    "        if sent[-1].split()[-1]==\"eol\":\n",
    "            break\n",
    "    \n",
    "    #print(\"\\n\")\n",
    "\n",
    "print(\" \".join(sent))\n",
    "    \n",
    "    \n",
    "#     print(\" \".join(sent[1:]).replace(\"eol\",\"\"))\n",
    "#     if(sent[-1]==\"eol\"):\n",
    "#         sent=[sent[-2]]\n",
    "        \n",
    "#     else:\n",
    "#         sent=[sent[-1]]\n",
    "    \n",
    "            \n",
    "#     for near in oen[:close]:\n",
    "#         print(keys[near[0]],count_dict[keys[near[0]]][1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(doc,b,U,d,H):\n",
    "    x = encode_doc(doc)\n",
    "    _h= layer1(x,H,d)\n",
    "    y = b+U*_h\n",
    "    \n",
    "    return dy.softmax(y)\n",
    "\n",
    "\n",
    "def layer1(x,H,d):\n",
    "    _H = dy.parameter(H)\n",
    "    _d = dy.parameter(d)\n",
    "    \n",
    "    return dy.tanh(_H * x + _d)\n",
    "\n",
    "def encode_doc(doc):\n",
    "    doc2=[]\n",
    "    embs=[]\n",
    "    for w in doc:\n",
    "        try:\n",
    "            doc2.append(w2i[w])\n",
    "        except:\n",
    "            doc2.append(w2i[\"mmeann\"])\n",
    "    for idx in doc2:\n",
    "        embs.append(E[idx])\n",
    "    \n",
    "    return dy.esum(embs)\n",
    "\n",
    "\n",
    "def do_loss(probs, next_word, w2v):\n",
    "    try:\n",
    "        next_word_vector = w2v[next_word]\n",
    "    except:\n",
    "        next_word_vector = w2v[\"mmeann\"]\n",
    "\n",
    "    return -dy.log(dy.pick_batch_elem(probs,next_word_vector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-50e68039bd56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-91e61f11da55>\u001b[0m in \u001b[0;36mdo_loss\u001b[0;34m(probs, next_word, w2v)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mnext_word_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2v\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mmeann\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpick_batch_elem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnext_word_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m_dynet.pyx\u001b[0m in \u001b[0;36m_dynet.pick_batch_elem\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "model = dy.Model()\n",
    "\n",
    "trainer = dy.SimpleSGDTrainer(model)\n",
    "\n",
    "h = 100 # HiddenUnit\n",
    "m = vectorDim\n",
    "OutUnit = 100\n",
    "\n",
    "H = model.add_parameters((h, m))\n",
    "d = model.add_parameters(h)\n",
    "U = model.add_parameters((m, h))\n",
    "b = model.add_parameters(m)\n",
    "\n",
    "E = model.add_lookup_parameters((len(w2i), HiddenDim),init =  embedding )\n",
    "\n",
    "for p in poems:\n",
    "    for gram in p:\n",
    "        dy.renew_cg()\n",
    "        probs = predict_labels(gram[:ngram-1],b,U,d,H)\n",
    "        \n",
    "        loss = do_loss(probs,gram[-1],w2v)\n",
    "        loss.forward()\n",
    "        loss.backward()\n",
    "        trainer.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "LookupParameter /_4\n",
      "(400000, 50)\n",
      "expression 9/0\n",
      "expression 10/0\n"
     ]
    }
   ],
   "source": [
    "print(len(w2i))\n",
    "print(E)\n",
    "print(E.shape())\n",
    "print(E[0])\n",
    "print(E[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit418366e376b14bef830f0db0b5287b85"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
